{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import numpy as np\n",
    "from nltk.corpus import cmudict\n",
    "pronounciations = cmudict.dict()\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_names(text_words, nnps):\n",
    "    text_bigrams = list(nltk.bigrams(text_words))\n",
    "    names = [bigram for bigram in text_bigrams if bigram[0] in nnps\n",
    "             and bigram[1] in nnps]\n",
    "    return names\n",
    "\n",
    "def find_topic(text):\n",
    "    if type(text) == list:\n",
    "        text_words = nltk.word_tokenize(text[0])\n",
    "        for n in range(1,len(text)):\n",
    "            text_words += nltk.word_tokenize(text[n])\n",
    "            \n",
    "    elif type(text) == str:\n",
    "        text_words = nltk.word_tokenize(text)\n",
    "    text_pos = nltk.pos_tag(text_words)\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text_pos_minus_stop = [(word, pos) for (word,pos) in text_pos if \n",
    "                           word not in stop_words]\n",
    "    NNPs = [word for (word,pos) in text_pos_minus_stop if pos=='NNP']\n",
    "    names = find_names(text_words, NNPs)\n",
    "    most_common = nltk.FreqDist(NNPs).most_common(1)[0][0]\n",
    "    if most_common in [name1 for (name1,name2) in names]:\n",
    "        for name in names:\n",
    "            if most_common == name[0]:\n",
    "                return name\n",
    "    if most_common in [name2 for (name1,name2) in names]:\n",
    "        for name in names:\n",
    "            if most_common == name[1]:\n",
    "                return name\n",
    "    return most_common\n",
    "\n",
    "def NPR_text(id_number):\n",
    "    html_page = requests.get('https://text.npr.org/s.php?sId={}'.format(id_number))\n",
    "    soup = BeautifulSoup(html_page.content, 'html.parser')\n",
    "    text = soup.text.split('\\n')[15:-10]\n",
    "    return text\n",
    "\n",
    "def text_wordify(text):\n",
    "    text_tokens = nltk.word_tokenize(text[0])\n",
    "    for n in range(1,len(text)):\n",
    "        text_tokens += nltk.word_tokenize(text[n])\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text_pos = nltk.pos_tag(text_tokens)\n",
    "    text_words = [(word,pos) for (word, pos) in text_pos if word not in stop_words]\n",
    "    return text_words\n",
    "\n",
    "def syllable_count(word):\n",
    "    count = 0\n",
    "    pron = pronounciations[word.lower()][0]\n",
    "    for syl in pron:\n",
    "        if syl[-1].isdigit():\n",
    "            count +=1\n",
    "    return count\n",
    "\n",
    "def find_nouns_adjs(text_words):\n",
    "    noun_adj_tags = ['NN','NNS','JJ','JJR','JJS']\n",
    "    noun_adjs = [word for (word,pos) in text_words if pos in noun_adj_tags]\n",
    "    wnl = nltk.WordNetLemmatizer()\n",
    "    lemmas = [wnl.lemmatize(word) for word in noun_adjs]\n",
    "    return lemmas\n",
    "\n",
    "def create_syllable_lists(lemmas):\n",
    "    syllables = {1:[],2:[],3:[],4:[],5:[]}\n",
    "    freq_dict =nltk.FreqDist(lemmas)\n",
    "    lemmas = list(set(lemmas))\n",
    "    for word in lemmas:\n",
    "        try:\n",
    "            syllables[syllable_count(word)].append((word,freq_dict[word]))\n",
    "        except:\n",
    "            pass\n",
    "    return syllables\n",
    "\n",
    "\n",
    "def rand_word(cfd):\n",
    "    cum_dic = {}\n",
    "    count = 0\n",
    "    if len(cfd) == 1:\n",
    "        return cfd[0][0]\n",
    "    for word in cfd:\n",
    "        count += word[1]\n",
    "        cum_dic[word[0]] = count\n",
    "    rand_int = np.random.randint(1,count+1)\n",
    "    for word in cum_dic:\n",
    "        if cum_dic[word] > rand_int:\n",
    "            return word\n",
    "        \n",
    "def fill_out_line(num_syllables, syllable_dict, line = [], already_used=[]):\n",
    "    while num_syllables > 1:\n",
    "        find_syl = np.random.randint(1,6)\n",
    "        if find_syl <= num_syllables and len(syllable_dict[find_syl]) >0:\n",
    "            words = syllable_dict[find_syl]\n",
    "            new_word = rand_word(words)\n",
    "            if new_word not in already_used:\n",
    "                line.append(new_word)\n",
    "                already_used.append(new_word)\n",
    "            num_syllables -= find_syl\n",
    "    if num_syllables == 1:\n",
    "        words = syllable_dict[1]\n",
    "        new_word = rand_word(words)\n",
    "        if new_word not in already_used:\n",
    "            line.append(rand_word(words))\n",
    "            already_used.append(new_word)\n",
    "    return line, already_used\n",
    "\n",
    "def write_haiku(id_num):\n",
    "    text = NPR_text(id_num)\n",
    "    text_words = text_wordify(text)\n",
    "    topic = find_topic(text)\n",
    "    noun_adjs = find_nouns_adjs(text_words)\n",
    "    syllable_dict = create_syllable_lists(noun_adjs)\n",
    "    \n",
    "    if type(topic) !=str:\n",
    "        topic_syls = syllable_count(topic[0]) + syllable_count(topic[1])\n",
    "    else:\n",
    "        topic_syls = syllable_count(topic)\n",
    "    line_1 = []\n",
    "    line_2 = []\n",
    "    line_3 = []\n",
    "    already_used = []\n",
    "    if topic_syls <=5:\n",
    "        if type(topic) !=str:\n",
    "            line_1 += [word for word in topic]\n",
    "        else:\n",
    "            line_1.append(topic)\n",
    "        line_1, already_used = fill_out_line(5-topic_syls, syllable_dict, line_1, already_used)\n",
    "        line_2, already_used = fill_out_line(7, syllable_dict, line_2, already_used)\n",
    "        line_3, already_used = fill_out_line(5, syllable_dict, line_3, already_used)\n",
    "    elif topic_syls <8:\n",
    "        if type(topic) !=str:\n",
    "            line_2 += [word for word in topic]\n",
    "        else:\n",
    "            line_2.append(topic)\n",
    "        line_1, already_used = fill_out_line(5, syllable_dict, line_1, already_used)\n",
    "        line_2, already_used = fill_out_line(7-topic_syls, syllable_dict, line_2, already_used)\n",
    "        line_3, already_used = fill_out_line(5, syllable_dict, line_3, already_used)\n",
    "\n",
    "    return line_1, line_2, line_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Health', 'Care', 'People', 'short'],\n",
       " ['proponent', 'room', 'lobbying'],\n",
       " ['executive', 'health'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_haiku(735578519)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Iran', 'many', 'thing'],\n",
       " ['strong', 'sanction', 'sign', 'week', 'last'],\n",
       " ['financial', 'chance'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_haiku(735424158)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Iran', 'sanction', 'lot'],\n",
       " ['administration', 'speech', 'planned'],\n",
       " ['move', 'chance', 'nuclear'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_haiku(735424158)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['letter', 'interview'],\n",
       " ['Treasury', 'Department', 'note'],\n",
       " ['new', 'indication'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_haiku(735525569)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['China', 'trade', 'fact', 'trade'],\n",
       " ['national', 'month', 'wu', 'word'],\n",
       " ['least', 'education'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_haiku(735274808)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Islamic', 'Iran'],\n",
       " ['market', 'ambassador', 'drone'],\n",
       " ['country', 'power', 'oil'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_haiku(735379562)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Health', 'Care', 'insurer'],\n",
       " ['laboratory', 'care', 'People'],\n",
       " ['many', 'hospital'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_haiku(735578519)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
