{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk #NLTK will do the lion's share of the work here\n",
    "from bs4 import BeautifulSoup #this is only to scrape the NPR plain text site, everything else just takes any text\n",
    "import requests\n",
    "import numpy as np\n",
    "from nltk.corpus import cmudict\n",
    "pronounciations = cmudict.dict() #the NLTK pronunciation dictionary\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first thing is just to get text, the NPR plain text site is easy to work with, so that's what I'm using here\n",
    "#Their article urls all follow the same pattern, so this takes in an article id number \n",
    "def NPR_text(id_number):\n",
    "    html_page = requests.get('https://text.npr.org/s.php?sId={}'.format(id_number))\n",
    "    soup = BeautifulSoup(html_page.content, 'html.parser')\n",
    "    text = soup.text.split('\\n')[15:-10]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first task was to make a guess at a 'topic' for any piece of text. In this case, it's more like a 'key-word' than a topic. The notion is that the text is likely to be about a proper noun, a person or institution, say, and that this proper noun will appear disproportionally often in the text. So, I have a function that simply parses the text for part of speech and returns the most common proper noun.\n",
    "\n",
    "This being my first project, I simply used the built in NLTK POS tagger, the notion being that I would later return to it and build my own tagger that could be inserted into this in place of the NLTK tagger. I've since started to use SpaCy and realize that a) SpaCy's tagger is better and faster both than NLTK and anything I'm likely to make and b) that Spacy would have solved another of my issues. So everything after this is going to be Spacy based.\n",
    "\n",
    "One of the quirks about doing this with NLTK was that names would get split up into two tokens, and NLTK wouldn't/couldn't keep track of names together as entities. For instance, a news article about Trump might refer to 'Trump' or 'Donald Trump'. If you counted up all the tagged proper nouns in the text, it would return 'Trump' as the most often used, but it wouldn't know to return the full name. Hence in this case I built a little helper funtion to look for 'names', which I reasoned would be bigrams in which both words were tagged as proper nouns, so that I could search my list of names for the most common proper noun and return the full name if the most common proper noun was part of a name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_names(text_words, nnps):\n",
    "    text_bigrams = list(nltk.bigrams(text_words))\n",
    "    names = [bigram for bigram in text_bigrams if bigram[0] in nnps\n",
    "             and bigram[1] in nnps]\n",
    "    return names\n",
    "\n",
    "def find_topic(text):\n",
    "    if type(text) == list:\n",
    "        text_words = nltk.word_tokenize(text[0])\n",
    "        for n in range(1,len(text)):\n",
    "            text_words += nltk.word_tokenize(text[n])\n",
    "            \n",
    "    elif type(text) == str:\n",
    "        text_words = nltk.word_tokenize(text)\n",
    "    text_pos = nltk.pos_tag(text_words)\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text_pos_minus_stop = [(word, pos) for (word,pos) in text_pos if \n",
    "                           word not in stop_words]\n",
    "    NNPs = [word for (word,pos) in text_pos_minus_stop if pos=='NNP'] #NNP is just 'proper nouns'\n",
    "    names = find_names(text_words, NNPs)\n",
    "    most_common = nltk.FreqDist(NNPs).most_common(1)[0][0] \n",
    "    #NLTK has a convenient, built in frequency dictionary function\n",
    "    \n",
    "    #now to check if the most common proper noun is in a name, either first or last name\n",
    "    if most_common in [name1 for (name1,name2) in names]:\n",
    "        for name in names:\n",
    "            if most_common == name[0]:\n",
    "                return name\n",
    "    if most_common in [name2 for (name1,name2) in names]:\n",
    "        for name in names:\n",
    "            if most_common == name[1]:\n",
    "                return name\n",
    "    return most_common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next a series of functions to create working dictionaries from which to pull words for the haiku. \n",
    "1. tokenize the text, remove stop words, parse for part of speach\n",
    "2. query the built in NLTK pronunciation dicitonary to calculate number of syllables\n",
    "3. Return the lemmas of all the nouns and adjectives in the text, these will be the basis of the haiku to avoid tense/morphological issues\n",
    "4. create a master syllable dictionary, indexed by number of syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_wordify(text):\n",
    "    text_tokens = nltk.word_tokenize(text[0])\n",
    "    for n in range(1,len(text)):\n",
    "        text_tokens += nltk.word_tokenize(text[n])\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text_pos = nltk.pos_tag(text_tokens)\n",
    "    text_words = [(word,pos) for (word, pos) in text_pos if word not in stop_words]\n",
    "    return text_words\n",
    "\n",
    "#The NLTK pronunciation dictionary returns a list of phonemes for a given word, finding the number of syllables\n",
    "#for a given word isn't intuitive. 'lay', 'play' and 'splay' all have one syllable, but would be different length\n",
    "#lists in the pronunciation dictionary. Each syllable comes with a stress mark which is a number 0, 1 or 2, \n",
    "#so counting these digits will tell you how many syllables a word is.\n",
    "def syllable_count(word):\n",
    "    count = 0\n",
    "    pron = pronounciations[word.lower()][0]\n",
    "    for syl in pron:\n",
    "        if syl[-1].isdigit():\n",
    "            count +=1\n",
    "    return count\n",
    "\n",
    "def find_nouns_adjs(text_words):\n",
    "    noun_adj_tags = ['NN','NNS','JJ','JJR','JJS']\n",
    "    noun_adjs = [word for (word,pos) in text_words if pos in noun_adj_tags]\n",
    "    wnl = nltk.WordNetLemmatizer()\n",
    "    lemmas = [wnl.lemmatize(word) for word in noun_adjs]\n",
    "    return lemmas\n",
    "\n",
    "def create_syllable_lists(lemmas):\n",
    "    syllables = {1:[],2:[],3:[],4:[],5:[]}\n",
    "    freq_dict =nltk.FreqDist(lemmas)\n",
    "    lemmas = list(set(lemmas))\n",
    "    for word in lemmas:\n",
    "        try:\n",
    "            syllables[syllable_count(word)].append((word,freq_dict[word]))\n",
    "        except:\n",
    "            pass\n",
    "    return syllables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the functions that actually build the haiku:\n",
    "1. select a random word, weighted by how often the word appears\n",
    "2. fill out a line in the haiku by taking a number of syllables and filling it with random words until you run out of syllables\n",
    "3. finally, write the haiku, inserting the 'topic' into line 1 (or line two if it is 6 or 7 syllables long) and filling out the rest of the haiku from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_word(cfd):\n",
    "    cum_dic = {}\n",
    "    count = 0\n",
    "    if len(cfd) == 1:\n",
    "        return cfd[0][0]\n",
    "    for word in cfd:\n",
    "        count += word[1]\n",
    "        cum_dic[word[0]] = count\n",
    "    rand_int = np.random.randint(1,count+1)\n",
    "    for word in cum_dic:\n",
    "        if cum_dic[word] > rand_int:\n",
    "            return word\n",
    "        \n",
    "def fill_out_line(num_syllables, syllable_dict, line = [], already_used=[]):\n",
    "    while num_syllables > 1:\n",
    "        find_syl = np.random.randint(1,6)\n",
    "        if find_syl <= num_syllables and len(syllable_dict[find_syl]) >0:\n",
    "            words = syllable_dict[find_syl]\n",
    "            new_word = rand_word(words)\n",
    "            if new_word not in already_used: #make sure we don't use the same word more than once\n",
    "                line.append(new_word)\n",
    "                already_used.append(new_word)\n",
    "            num_syllables -= find_syl\n",
    "    if num_syllables == 1:\n",
    "        words = syllable_dict[1]\n",
    "        new_word = rand_word(words)\n",
    "        if new_word not in already_used:\n",
    "            line.append(rand_word(words))\n",
    "            already_used.append(new_word)\n",
    "    return line, already_used #also return the already used list so we can use the same list across multiple lines\n",
    "\n",
    "def write_haiku(id_num):\n",
    "    text = NPR_text(id_num)\n",
    "    text_words = text_wordify(text)\n",
    "    topic = find_topic(text)\n",
    "    noun_adjs = find_nouns_adjs(text_words)\n",
    "    syllable_dict = create_syllable_lists(noun_adjs)\n",
    "    \n",
    "    if type(topic) !=str:\n",
    "        topic_syls = syllable_count(topic[0]) + syllable_count(topic[1])\n",
    "    else:\n",
    "        topic_syls = syllable_count(topic)\n",
    "    line_1 = []\n",
    "    line_2 = []\n",
    "    line_3 = []\n",
    "    already_used = []\n",
    "    if topic_syls <=5:\n",
    "        if type(topic) !=str:\n",
    "            line_1 += [word for word in topic]\n",
    "        else:\n",
    "            line_1.append(topic)\n",
    "        line_1, already_used = fill_out_line(5-topic_syls, syllable_dict, line_1, already_used)\n",
    "        line_2, already_used = fill_out_line(7, syllable_dict, line_2, already_used)\n",
    "        line_3, already_used = fill_out_line(5, syllable_dict, line_3, already_used)\n",
    "    elif topic_syls <8:\n",
    "        if type(topic) !=str:\n",
    "            line_2 += [word for word in topic]\n",
    "        else:\n",
    "            line_2.append(topic)\n",
    "        line_1, already_used = fill_out_line(5, syllable_dict, line_1, already_used)\n",
    "        line_2, already_used = fill_out_line(7-topic_syls, syllable_dict, line_2, already_used)\n",
    "        line_3, already_used = fill_out_line(5, syllable_dict, line_3, already_used)\n",
    "\n",
    "    return line_1, line_2, line_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Health', 'Care', 'People', 'short'],\n",
       " ['proponent', 'room', 'lobbying'],\n",
       " ['executive', 'health'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_haiku(735578519)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Iran', 'many', 'thing'],\n",
       " ['strong', 'sanction', 'sign', 'week', 'last'],\n",
       " ['financial', 'chance'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_haiku(735424158)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Iran', 'sanction', 'lot'],\n",
       " ['administration', 'speech', 'planned'],\n",
       " ['move', 'chance', 'nuclear'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_haiku(735424158)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['letter', 'interview'],\n",
       " ['Treasury', 'Department', 'note'],\n",
       " ['new', 'indication'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_haiku(735525569)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['China', 'trade', 'fact', 'trade'],\n",
       " ['national', 'month', 'wu', 'word'],\n",
       " ['least', 'education'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_haiku(735274808)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Islamic', 'Iran'],\n",
       " ['market', 'ambassador', 'drone'],\n",
       " ['country', 'power', 'oil'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_haiku(735379562)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Health', 'Care', 'insurer'],\n",
       " ['laboratory', 'care', 'People'],\n",
       " ['many', 'hospital'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_haiku(735578519)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Louisiana'],\n",
       " ['hurricane', 'morning', 'part', 'storm'],\n",
       " ['emergency', 'close'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_haiku(741382999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Louisiana'], ['evacuation', 'road', 'line'], ['rain', 'infrastructure'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_haiku(741382999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
