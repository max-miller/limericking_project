{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NPR_text(id_number):\n",
    "    html_page = requests.get('https://text.npr.org/s.php?sId={}'.format(id_number))\n",
    "    soup = BeautifulSoup(html_page.content, 'html.parser')\n",
    "    text = soup.text.split('\\n')[15:-10]\n",
    "    tokens = []\n",
    "    for block in text:\n",
    "        tokens += nltk.word_tokenize(block)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple text summarizer, comparing sentence vectors with cosine similarity and returning the sentense that seem most similar to the others in the text. First a walk through of the different parts, then wrapped in a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = vectorizer.fit_transform(NPR_text(745418569))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22, 354)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(transformed.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "similars = cosine_similarity(transformed,transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "averages = []\n",
    "for i in similars:\n",
    "    averages.append(i.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_indices = sorted(list(np.argsort(averages)[-4:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 7, 12, 14]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fast-fashion giant pledged that by 2025, all of its eight brands will only use cotton, linen and polyester that's organic, sustainable or recycled, which is 90% of the raw materials its uses. CEO and executive chairman Pablo Isla said that renewable sources will power 80% of the energy consumed by the conglomerate's distribution centers, offices and stores. It also plans to transition to zero landfill waste.\n",
      "Cline says that even if Zara is using materials that are more ethically sourced or have a lower environmental impact, the vast majority of the carbon footprint of fashion comes from the manufacturers who supply brands with their materials. When a business is built on a fast turnover of styles, making those products still swallows a lot of energy, regardless of whether it's using organic cotton or selling products in more eco-efficient stores.\n",
      "\"The fashion industry isn't actually just one industry, it's a whole raft of other industries that are used and exploited to deliver the garments that we're wearing now,\" he says in an interview with NPR's All Things Considered. \n",
      "\"They're acting overly confident about a subject that we're still figuring out,\" she says. \"We are still gathering data. We are still figuring out best practices. So for Zara to kind of come out of the gate and say we're going to be sustainable by 2025 belies the long road ahead of us that we have on sustainability and fashion.\"\n"
     ]
    }
   ],
   "source": [
    "text = NPR_text(745418569)\n",
    "for n in sent_indices:\n",
    "    print(text[n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example text scraped from the NYT was generated here. Finding text not really the focus of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyt_api = 'key goes here'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyt_url = f'https://api.nytimes.com/svc/topstories/v2/science.json?api-key={nyt_api}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = requests.get(nyt_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_articles_text(url):\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    req = session.get(url)\n",
    "    soup = BeautifulSoup(req.text, 'lxml')\n",
    "\n",
    "    paragraph_tags = soup.find_all('p', class_= 'css-exrw3m evys1bk0')\n",
    "    if paragraph_tags == []:\n",
    "        paragraph_tags = soup.find_all('p', itemprop = 'articleBody')\n",
    "\n",
    "    article = ''\n",
    "    for p in paragraph_tags:\n",
    "        article = article + ' ' + p.get_text()\n",
    "\n",
    "    # Clean article replacing unicode characters\n",
    "    article = article.replace(u'\\u2018', u\"'\").replace(u'\\u2019', u\"'\").replace(u'\\u201c', u'\"').replace(u'\\u201d', u'\"')\n",
    "\n",
    "    return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = resp.json()['results'][9]['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = scrape_articles_text(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The chief executive of Novartis on Wednesday defended the company's decision to delay telling the Food and Drug Administration about manipulated data involving its $2.1 million gene therapy treatment, saying that it \"thoroughly, aggressively\" investigated the issue and that patient safety was never threatened.\n",
      "Vas Narasimhan, the chief executive, also indicated in a call with investors that the company was forcing out a small number of scientists who were involved in the manipulated data.  \n",
      "The F.D.A. on Tuesday issued an unusual public rebuke of Novartis for failing to report the falsified data before its gene therapy treatment, Zolgensma, was approved in May, even though the company had known about the issue since March.\n",
      "The agency said it was continuing to investigate and the company could face civil or criminal penalties.\n",
      "Novartis and the F.D.A. have said the falsified data did not affect the safety, quality or efficacy of Zolgensma, a therapy that treats a rare genetic disease known as spinal muscular atrophy.\n",
      "The product will remain on the market; about 400 babies a year are born with the disorder in the United States.  \n",
      "The revelations represented a fresh controversy for Novartis and Mr. Narasimhan, who became chief executive in 2018 with a mission to restore the company's reputation following a series of crises involving allegations of price-fixing in its generic-drug unit, improper marketing and an earlier data manipulation scandal in its Japanese unit.\n",
      "Last year, the company also came under scrutiny for its decision to hire Michael Cohen, President Trump's former personal lawyer, as a consultant.\n",
      "[Like the Science Times page on Facebook.\n",
      "|\n",
      "Sign up for the Science Times newsletter.]\n",
      "The data manipulation also threatened to tarnish the image of Zolgensma, only the second gene therapy treatment to be approved by the F.D.A., and one that is being closely watched as dozens of other gene therapies are in the works.\n",
      "Zolgensma is under review by regulators in Europe and Japan.\n",
      "Mr. Narasimhan said he did not expect the disclosure about the data to affect those applications.\n",
      "\"\n",
      "We are committed to rebuilding trust with society, deeply for me, personally, all the way through the broader organization,\" he said, while acknowledging that \"it's a long, long road, and sometimes it's bumpy.\n",
      "\"\n",
      "The last-minute call appeared to be an effort to reassure investors about any threat to one of the company's most promising new products.\n",
      "And at least one law firm was already advertising a potential shareholder lawsuit by Tuesday evening, after Novartis's stock fell nearly 3 percent.\n",
      "Shares were up slightly by midday on Wednesday.\n",
      "Mr. Narasimhan and other company executives said Wednesday that they had waited to conduct their own investigation before reporting what they had found to the agency, and that it was not an intentional effort to deceive federal regulators.\n",
      "The company investigation, they said, showed that the falsified data was limited to experiments on mice used in early phases of the research, and that the tests in question were discontinued last summer.  \n",
      "An F.D.A. inspection report — dated July 24 to Aug. 2, 2019, and made public on Tuesday — pointed out faulty record-keeping by the company, as well as improper procedures in quality control in gathering data on the mice.\n",
      "Dave Lennon, president of AveXis, the Novartis unit that developed Zolgensma, said the episode appeared to be isolated.\n",
      "\"\n",
      "We do believe this was an issue that was historic in nature, and relegated to a few individuals within the company.\n",
      "\"\n",
      "Even so, the disclosure evoked an ominous prediction from Dr. Scott Gottlieb, the recently departed commissioner of the F.D.A., who said on Twitter:\n",
      "\"I suspect there will be wrongdoers here.\n",
      "And consequences.\n",
      "\"\n",
      "[Like the Science Times page on Facebook.\n",
      "|\n",
      "Sign up for the Science Times newsletter.]\n"
     ]
    }
   ],
   "source": [
    "for sent in text.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_sent_parser(text):\n",
    "    temp_sentences = []\n",
    "    #using SpaCy to parse the text for sentences\n",
    "    text = nlp(text)\n",
    "    #unfortunately will need to reconvert back into strings\n",
    "    for sent in text.sents:\n",
    "        temp = str()\n",
    "        for token in sent:\n",
    "            temp += token.text\n",
    "            temp += ' '\n",
    "        temp_sentences.append(temp)\n",
    "    #Lastly cutting out repeated sentences, to get rid of things like 'subscribe here'\n",
    "    #That come up depending on where you've scraped the text\n",
    "    indices_to_avoid = []\n",
    "    #first identify where the repeated sentences are\n",
    "    for n in range(0,len(temp_sentences)):\n",
    "        for i in range(n+1,len(temp_sentences)):\n",
    "            if temp_sentences[n] == temp_sentences[i]:\n",
    "                indices_to_avoid.append(n)\n",
    "                indices_to_avoid.append(i)\n",
    "    sentences = []\n",
    "    #and then exclude them\n",
    "    for n in range(0,len(temp_sentences)):\n",
    "        if n not in indices_to_avoid:\n",
    "            sentences.append(temp_sentences[n])\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_list = text_sent_parser(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_comparer(sentence_list):\n",
    "    #sklearn vectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    transformed = vectorizer.fit_transform(sentence_list)\n",
    "    #Using cosine similarity on the whole list of vectors produces a matrix\n",
    "    #where each sentence's vector is compared to every other sentence\n",
    "    similars = cosine_similarity(transformed,transformed)\n",
    "    #now we collapse into single values\n",
    "    averages = []\n",
    "    for i in similars:\n",
    "        averages.append(i.mean())\n",
    "    #And return the indices of the highest values\n",
    "    #Sorted by index, so that our summary is chronological\n",
    "    n_sents = int(len(sentence_list)**.5)\n",
    "    sent_indices = sorted(list(np.argsort(averages)[-n_sents:]))\n",
    "    return sent_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 8, 16]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_comparer(sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarizer(text):\n",
    "    sentence_list = text_sent_parser(text)\n",
    "    summary_indices = sent_comparer(sentence_list)\n",
    "    return [sentence_list[n] for n in summary_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['  The chief executive of Novartis on Wednesday defended the company \\'s decision to delay telling the Food and Drug Administration about manipulated data involving its $ 2.1 million gene therapy treatment , saying that it \" thoroughly , aggressively \" investigated the issue and that patient safety was never threatened . ',\n",
       " 'The F.D.A. on Tuesday issued an unusual public rebuke of Novartis for failing to report the falsified data before its gene therapy treatment , Zolgensma , was approved in May , even though the company had known about the issue since March . ',\n",
       " 'The data manipulation also threatened to tarnish the image of Zolgensma , only the second gene therapy treatment to be approved by the F.D.A. , and one that is being closely watched as dozens of other gene therapies are in the works . ',\n",
       " 'The company investigation , they said , showed that the falsified data was limited to experiments on mice used in early phases of the research , and that the tests in question were discontinued last summer .   ']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = summarizer(text)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = summary[0]\n",
    "test_sentence = nlp(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  The chief executive of Novartis on Wednesday defended the company 's decision to delay telling the Food and Drug Administration about manipulated data involving its $ 2.1 million gene therapy treatment , saying that it \" thoroughly , aggressively \" investigated the issue and that patient safety was never threatened . "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word:                 POS: SPACE      _SP           None\n",
      "word: The             POS: DET        DT    det     determiner\n",
      "word: chief           POS: ADJ        JJ    amod    adjectival modifier\n",
      "word: executive       POS: NOUN       NN    nsubj   nominal subject\n",
      "word: of              POS: ADP        IN    prep    prepositional modifier\n",
      "word: Novartis        POS: PROPN      NNP   pobj    object of preposition\n",
      "word: on              POS: ADP        IN    prep    prepositional modifier\n",
      "word: Wednesday       POS: PROPN      NNP   pobj    object of preposition\n",
      "word: defended        POS: VERB       VBD   ROOT    None\n",
      "word: the             POS: DET        DT    det     determiner\n",
      "word: company         POS: NOUN       NN    poss    possession modifier\n",
      "word: 's              POS: PART       POS   case    case marking\n",
      "word: decision        POS: NOUN       NN    dobj    direct object\n",
      "word: to              POS: PART       TO    aux     auxiliary\n",
      "word: delay           POS: VERB       VB    acl     clausal modifier of noun (adjectival clause)\n",
      "word: telling         POS: VERB       VBG   xcomp   open clausal complement\n",
      "word: the             POS: DET        DT    det     determiner\n",
      "word: Food            POS: PROPN      NNP   nmod    modifier of nominal\n",
      "word: and             POS: CCONJ      CC    cc      coordinating conjunction\n",
      "word: Drug            POS: PROPN      NNP   conj    conjunct\n",
      "word: Administration  POS: PROPN      NNP   dobj    direct object\n",
      "word: about           POS: ADP        IN    prep    prepositional modifier\n",
      "word: manipulated     POS: VERB       VBN   amod    adjectival modifier\n",
      "word: data            POS: NOUN       NNS   pobj    object of preposition\n",
      "word: involving       POS: VERB       VBG   acl     clausal modifier of noun (adjectival clause)\n",
      "word: its             POS: DET        PRP$  poss    possession modifier\n",
      "word: $               POS: SYM        $     quantmod modifier of quantifier\n",
      "word: 2.1             POS: NUM        CD    compound compound\n",
      "word: million         POS: NUM        CD    nummod  numeric modifier\n",
      "word: gene            POS: NOUN       NN    compound compound\n",
      "word: therapy         POS: NOUN       NN    compound compound\n",
      "word: treatment       POS: NOUN       NN    dobj    direct object\n",
      "word: ,               POS: PUNCT      ,     punct   punctuation\n",
      "word: saying          POS: VERB       VBG   advcl   adverbial clause modifier\n",
      "word: that            POS: ADP        IN    mark    marker\n",
      "word: it              POS: PRON       PRP   nsubj   nominal subject\n",
      "word: \"               POS: PUNCT      ``    punct   punctuation\n",
      "word: thoroughly      POS: ADV        RB    advmod  adverbial modifier\n",
      "word: ,               POS: PUNCT      ,     punct   punctuation\n",
      "word: aggressively    POS: ADV        RB    advmod  adverbial modifier\n",
      "word: \"               POS: PUNCT      ``    punct   punctuation\n",
      "word: investigated    POS: VERB       VBD   ccomp   clausal complement\n",
      "word: the             POS: DET        DT    det     determiner\n",
      "word: issue           POS: NOUN       NN    dobj    direct object\n",
      "word: and             POS: CCONJ      CC    cc      coordinating conjunction\n",
      "word: that            POS: DET        DT    mark    marker\n",
      "word: patient         POS: ADJ        JJ    amod    adjectival modifier\n",
      "word: safety          POS: NOUN       NN    nsubjpass nominal subject (passive)\n",
      "word: was             POS: VERB       VBD   auxpass auxiliary (passive)\n",
      "word: never           POS: ADV        RB    neg     negation modifier\n",
      "word: threatened      POS: VERB       VBN   conj    conjunct\n",
      "word: .               POS: PUNCT      .     punct   punctuation\n"
     ]
    }
   ],
   "source": [
    "for token in test_sentence:\n",
    "    print(f'word: {token.text:{15}} POS: {token.pos_:{10}} {token.tag_:{5}} \n",
    "          {token.dep_:{7}} {spacy.explain(token.dep_)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_1 = summarizer(' '.join(NPR_text(748416223)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NPR.org , August 7 , 2019 · Scientists are adding a new creature to a list of giant , prehistoric animals that were previously unknown : The Heracles inexpectus , a supersize parrot , estimated to have been as tall as a small human child , was discovered by Australian researchers in New Zealand , according to a study published in Biology Letters .   ',\n",
       " 'Michael Archer , a paleontologist at the University of New South Wales who was also involved in the study , remarked that the bird \\'s stature would make it \" able to pick the lint out of your bellybutton . ',\n",
       " 'Allison Shultz , associate curator of ornithology at the Natural History Museum of Los Angeles County , told NPR the Heracles \\' story fits the pattern of what has happened to bird species over the ages : \" They get to an island , lose the ability to fly and get really big . ',\n",
       " 'Likewise there was a crocodile in the fauna , but one expects a parrot to be more clever than those and unlikely to be caught very often , \" Worthy added . ']"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer(' '.join(NPR_text(749224941)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The panel of scientists looked at the climate change effects of agriculture , deforestation and other land use , such as harvesting peat and managing grasslands and wetlands . ',\n",
       " 'The new report adds weight and detail to a warning put out by the same panel of scientists last fall , in which they sounded the alarm about the inadequacy of the pledges countries have made so far to reduce greenhouse gas emissions .   ',\n",
       " 'At that time , the panel broadly suggested that farmland would need to shrink and forests would need to grow to keep Earth from getting more than 1.5 degrees Celsius hotter than it was in the preindustrial era . ',\n",
       " \"Scientists say the only way to achieve that reduction is to significantly increase the amount of land that 's covered in trees and other vegetation and significantly reduce the amount of methane and other greenhouse gases that come from raising livestock such as cows , sheep and goats . \",\n",
       " 'The U.N. panel is the latest group of experts to grapple with a global conundrum : how to reduce greenhouse gas emissions from agriculture , deforestation and other land use without creating food shortages or displacing people whose livelihoods rely on practices that are unsustainable globally . ']"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_1 = nlp(summary_1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Once you have a good summary, finding a specific subject, or subject chunk becomes pretty easy\n",
    "#Simply use spaCy to identify the grammatical subject of your summary sentence\n",
    "def find_subj(nlp_text):\n",
    "    for token in nlp_text:\n",
    "        if token.dep_ == 'nsubj':\n",
    "            return token\n",
    "\n",
    "\n",
    "def complete_subject_chunk(nlp_text):\n",
    "    subj = find_subj(nlp_text)\n",
    "    span_start = subj.left_edge.i\n",
    "    span_end = subj.right_edge.i\n",
    "    return nlp_text[span_start:span_end+1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "panel of"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_1[1:subj.right_edge.i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The panel of scientists"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subj_chunk = complete_subject_chunk(sent_1)\n",
    "subj_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now to adjust my haiku generator, getting vocab only from the summary\n",
    "def find_vocab(sentence_list):\n",
    "    subj = find_subj(nlp(sentence_list[0]))\n",
    "    subj_chunk = complete_subject_chunk(nlp(sentence_list[0]))\n",
    "    \n",
    "    complete_text = nlp(' '.join(sentence_list))\n",
    "    vocab = []\n",
    "    for token in complete_text:\n",
    "        if token not in subj_chunk:\n",
    "            if token.pos_ == 'ADJ':\n",
    "                vocab.append(token.text.lower())\n",
    "            if token.pos_ == 'NOUN':\n",
    "                vocab.append(token.text.lower())\n",
    "    vocab = list(set(vocab))\n",
    "    return subj.text, subj_chunk.text, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj, subj_chunk, vocab = find_vocab(summary_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import cmudict\n",
    "pronounciations = cmudict.dict()\n",
    "\n",
    "def syllable_count(word):\n",
    "    word = word.split()\n",
    "    count = 0\n",
    "    for item in word:\n",
    "        pron = pronounciations[item.lower()][0]\n",
    "        for syl in pron:\n",
    "            if syl[-1].isdigit():\n",
    "                count +=1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'panel', 'of', 'scientists']"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subj_chunk.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syllable_count(subj_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def master_dict_maker(vocab):\n",
    "    master_dict = {}\n",
    "    for word in vocab:\n",
    "        try:\n",
    "            count = syllable_count(word)\n",
    "            master_dict[count] = master_dict.get(count,[]) + [word]\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return master_dict\n",
    "\n",
    "\n",
    "def find_word_for_line(max_syllables, master_dict):\n",
    "    word_list = []\n",
    "    for n in master_dict.keys():\n",
    "        if n <= max_syllables:\n",
    "            word_list = word_list + master_dict[n]\n",
    "\n",
    "    word = word_list[np.random.randint(1,len(word_list)+1)]\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "syllable_dict = master_dict_maker(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_out_line(num_syllables, syllable_dict, line = [], already_used=[]):\n",
    "    while num_syllables > 0:\n",
    "        new_word = find_word_for_line(num_syllables, syllable_dict)\n",
    "        if new_word not in already_used: #make sure we don't use the same word more than once\n",
    "            line.append(new_word)\n",
    "            already_used.append(new_word)\n",
    "            num_syllables -= syllable_count(new_word)\n",
    "        \n",
    "    return line, already_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['land', 'amount', 'shortages', 'cows'],\n",
       " ['land', 'amount', 'shortages', 'cows'])"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_out_line(7, syllable_dict, line = [], already_used=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_haiku(id_num):\n",
    "    summary = summarizer(' '.join(NPR_text(id_num)))\n",
    "    subj, subj_chunk, vocab = find_vocab(summary)\n",
    "    syllable_dict = master_dict_maker(vocab)\n",
    "    \n",
    "    \n",
    "    line_1 = []\n",
    "    line_2 = []\n",
    "    line_3 = []\n",
    "    already_used = []\n",
    "    if syllable_count(subj_chunk) <=5:\n",
    "        line_1.append(subj_chunk)\n",
    "        line_1, already_used = fill_out_line(5-syllable_count(subj_chunk), syllable_dict, line_1, already_used)\n",
    "        line_2, already_used = fill_out_line(7, syllable_dict, line_2, already_used)\n",
    "        line_3, already_used = fill_out_line(5, syllable_dict, line_3, already_used)\n",
    "        \n",
    "    elif syllable_count(subj_chunk) <=7:\n",
    "        line_2.append(subj_chunk)\n",
    "        line_1, already_used = fill_out_line(5, syllable_dict, line_1, already_used)\n",
    "        line_2, already_used = fill_out_line(7-syllable_count(subj_chunk), syllable_dict, line_2, already_used)\n",
    "        line_3, already_used = fill_out_line(5, syllable_dict, line_3, already_used)\n",
    "    elif syllable_count(subj) <=7:\n",
    "        line_2.append(subj)\n",
    "        line_1, already_used = fill_out_line(5, syllable_dict, line_1, already_used)\n",
    "        line_2, already_used = fill_out_line(7-syllable_count(subj), syllable_dict, line_2, already_used)\n",
    "        line_3, already_used = fill_out_line(5, syllable_dict, line_3, already_used)\n",
    "        \n",
    "\n",
    "\n",
    "    return line_1, line_2, line_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['sheep', 'global', 'need', 'food'],\n",
       " ['The panel of scientists'],\n",
       " ['new', 'effects', 'countries'])"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_haiku(748416223)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Scientists', 'small', 'lint'],\n",
       " ['bird', 'fauna', 'island', 'story'],\n",
       " ['ornithology'])"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_haiku(749224941)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
