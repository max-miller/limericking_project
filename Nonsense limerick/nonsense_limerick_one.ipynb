{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import cmudict\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json\n",
    "import string\n",
    "from newsapi import NewsApiClient\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from spacy import displacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NPR.org, July 8, 2019 · Before Jarret Stopforth ta'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pulling an example text from npr plain text site\n",
    "def NPR_text(id_number):\n",
    "    html_page = requests.get('https://text.npr.org/s.php?sId={}'.format(id_number))\n",
    "    soup = BeautifulSoup(html_page.content, 'html.parser')\n",
    "    text = soup.text.split('\\n')[15:-10]\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "text = NPR_text(737628786)\n",
    "text[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, from this we're going to create a nonsense limerick. Find rhymes, assemble a rhyme scheme, fill in lines with words that roughly scan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the pronunciations dictionary from NLTK - put into a more search friendly form as a dictionary with the\n",
    "#word in English as the key and its pronunciations as the value\n",
    "entries = nltk.corpus.cmudict.entries()\n",
    "pron_dict = {}\n",
    "for entry in entries:\n",
    "    pron_dict[entry[0]]=entry[1]\n",
    "    \n",
    "\n",
    "def find_rhymes(text):\n",
    "    text = nltk.word_tokenize(text)\n",
    "    text = [word.lower() for word in text]\n",
    "    text_set = list(set(text)) #to avoid duplicate words being rhymed with themselves\n",
    "    #The way NLTK tokenizes, it leaves these 's around, so need to remove them along with the normal stop words\n",
    "    stop_words = list(set(stopwords.words('english')))+[\"'s\"]  \n",
    "    text_set = [word for word in text_set if word not in stop_words] #removing stop words as well\n",
    "    pronunciations = []\n",
    "    #build a master list of all pronunciations that is easier to iterate over than the dictionary\n",
    "    for word in text_set:\n",
    "        try:\n",
    "            pronunciations.append((word,pron_dict[word]))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    #here is the actual matching\n",
    "    match_dict = {}\n",
    "    for n in range(0,len(pronunciations)): #given one word in the list\n",
    "        for i in range(n+1,len(pronunciations)): #we'll check it against all words that follow\n",
    "            if (pronunciations[n][1][1:] == pronunciations[i][1][-len(pronunciations[n][1][1:]):] or\n",
    "                    pronunciations[i][1][1:] == pronunciations[n][1][-len(pronunciations[i][1][1:]):]):\n",
    "                # need to cutt out super short rhymes - else you get things like 'is' rhyming\n",
    "                # with every word that ends with s\n",
    "                if ((len(pronunciations[n][1][1:]) > 2 or len(pronunciations[n][1]) ==3) and \n",
    "                    (len(pronunciations[i][1][1:]) > 2 or len(pronunciations[i][1]) ==2)):\n",
    "                    if len(pronunciations[n][1][1:]) < len(pronunciations[i][1][1:]): \n",
    "                        rhyme = ''.join(pronunciations[n][1][1:])\n",
    "                    else:\n",
    "                        rhyme = ''.join(pronunciations[i][1][1:])\n",
    "                    if len(rhyme) > 2:\n",
    "                    #if this rhyme has already been added to the dictionary of rhymes we can include it\n",
    "                        if rhyme in match_dict.keys(): \n",
    "                            match_dict[rhyme] = list(set(match_dict[rhyme]+\n",
    "                                                         [pronunciations[n][0],pronunciations[i][0]]))\n",
    "                        else: #if the dictionary doesn't have an entry for this rhyme, we put in a \n",
    "                            match_dict[rhyme] = [pronunciations[n][0],pronunciations[i][0]]\n",
    "\n",
    "    return match_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EY1KS': ['takes', 'makes'],\n",
       " 'EY1KIH0NG': ['waking', 'making'],\n",
       " 'AE1ND': ['demand', 'land'],\n",
       " 'AH1M': ['come', 'become'],\n",
       " 'EY1N': ['gain', 'campaign', 'maintain'],\n",
       " 'AW1ND': ['ground', 'compound', 'round', 'found'],\n",
       " 'RIH1NGK': ['drink', 'shrink'],\n",
       " 'EH1TAH0L': ['kettle', 'settle'],\n",
       " 'UW1D': ['food', 'brewed'],\n",
       " 'IH1NGKIH0NG': ['drinking', 'thinking'],\n",
       " 'AY1T': ['right', 'despite'],\n",
       " 'LIY0': ['early', 'exactly', 'really'],\n",
       " 'BAH0L': ['possible', 'able', 'global', 'suitable'],\n",
       " 'IH1TIH0NG': ['getting', 'spitting']}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rhymes = find_rhymes(text)\n",
    "rhymes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to assemble a corpus of words to use. That is, units that can be parsed for syllable/meter and then selected to be put into each line. A couple of challenges:\n",
    "\n",
    "1. Certain words are actually groups that need to be kept togeter, particularly names\n",
    "2. To avoid obvious tense/usage issues, it seems to make sense to only include certain parts of speech or to otherwise lemmatize the words\n",
    "3. What to do with stopwords: you obviously don't want to assemble an entire line of them by accident, but then you don't want to throw out words like 'the', 'there' or 'was'\n",
    "\n",
    "First step is finding proper nouns and names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function, this is going to take in a list of things labeled as 'proper nouns' and return the \n",
    "#names by looking for bigrams where the two proper nouns occur next to eachother.\n",
    "def find_names(text_words, nnps):\n",
    "    text_bigrams = list(nltk.bigrams(text_words))\n",
    "    names = [bigram for bigram in text_bigrams if bigram[0] in nnps\n",
    "             and bigram[1] in nnps]\n",
    "    names = list(set(names))\n",
    "    return names\n",
    "\n",
    "def find_proper_nouns(text):\n",
    "    text_words = nltk.word_tokenize(text)\n",
    "    text_pos = nltk.pos_tag(text_words)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text_pos_minus_stop = [(word, pos) for (word,pos) in text_pos if word not in stop_words]\n",
    "    NNPs = [word for (word,pos) in text_pos_minus_stop if pos=='NNP']\n",
    "    names = find_names(text_words, NNPs)\n",
    "    NNPs = list(set(NNPs))\n",
    "    for word in NNPs: #and here we remove the proper nouns that are part of the names list\n",
    "        if word in [name1 for (name1,name2) in names]+[name2 for (name1,name2) in names]:\n",
    "            NNPs.remove(word)\n",
    "    return NNPs + names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Soylent',\n",
       " 'Atomo',\n",
       " 'Kickstarter',\n",
       " 'Chobani',\n",
       " ']',\n",
       " 'Fire',\n",
       " 'Kettle',\n",
       " 'University',\n",
       " 'Taylor',\n",
       " 'July',\n",
       " 'Never',\n",
       " 'Washington',\n",
       " 'NPR.org',\n",
       " 'Helmer',\n",
       " 'Center',\n",
       " 'Tropical',\n",
       " 'Kleitsch',\n",
       " 'Graduate',\n",
       " 'New',\n",
       " 'North',\n",
       " 'U.S.',\n",
       " 'Drug',\n",
       " 'Oregon',\n",
       " ('Taylor', 'Moore'),\n",
       " ('North', 'Carolina'),\n",
       " ('Jarret', 'Stopforth'),\n",
       " ('Andy', 'Kleitsch'),\n",
       " ('Jodi', 'Helmer'),\n",
       " ('Tropical', 'Agriculture'),\n",
       " ('Drug', 'Administration'),\n",
       " ('Christopher', 'Hendon'),\n",
       " ('U.S.', 'Food'),\n",
       " ('New', 'Orleans'),\n",
       " ('International', 'Center')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_proper_nouns(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['takes',\n",
       " 'first',\n",
       " 'sip',\n",
       " 'coffee',\n",
       " 'adds',\n",
       " 'cream',\n",
       " 'sugar',\n",
       " 'mask',\n",
       " 'bitterness',\n",
       " 'thought',\n",
       " 'settle',\n",
       " 'regular',\n",
       " 'cup',\n",
       " 'joe',\n",
       " 'food',\n",
       " 'scientist',\n",
       " 'decided',\n",
       " 'reengineer',\n",
       " 'coffee',\n",
       " 'brewing',\n",
       " 'bitterness',\n",
       " '—',\n",
       " 'bean',\n",
       " 'started',\n",
       " 'thinking',\n",
       " 'able',\n",
       " 'break',\n",
       " 'coffee',\n",
       " 'core',\n",
       " 'components',\n",
       " 'look',\n",
       " 'optimize',\n",
       " 'explains',\n",
       " 'worked',\n",
       " 'food',\n",
       " 'brands',\n",
       " 'partnered',\n",
       " 'entrepreneur',\n",
       " 'launch',\n",
       " 'pair',\n",
       " 'turned',\n",
       " 'Seattle',\n",
       " 'garage',\n",
       " 'brewing',\n",
       " 'lab',\n",
       " 'spent',\n",
       " 'months',\n",
       " 'running',\n",
       " 'green',\n",
       " 'beans',\n",
       " 'roasted',\n",
       " 'beans',\n",
       " 'brewed',\n",
       " 'coffee',\n",
       " 'gas',\n",
       " 'liquid',\n",
       " 'chromatography',\n",
       " 'separate',\n",
       " 'catalog',\n",
       " 'compounds',\n",
       " 'coffee',\n",
       " 'create',\n",
       " 'product',\n",
       " 'color',\n",
       " 'aroma',\n",
       " 'flavor',\n",
       " 'mouthfeel',\n",
       " 'coffee',\n",
       " 'got',\n",
       " 'deeper',\n",
       " 'process',\n",
       " 'learned',\n",
       " 'threats',\n",
       " 'coffee',\n",
       " 'world',\n",
       " 'whole',\n",
       " '—',\n",
       " 'threats',\n",
       " 'environment',\n",
       " 'deforestation',\n",
       " 'global',\n",
       " 'warming',\n",
       " '[',\n",
       " 'devastating',\n",
       " 'fungus',\n",
       " 'called',\n",
       " 'rust',\n",
       " 'even',\n",
       " 'committed',\n",
       " 'making',\n",
       " 'consistently',\n",
       " 'great',\n",
       " 'coffee',\n",
       " 'also',\n",
       " 'better',\n",
       " 'environment',\n",
       " 'says',\n",
       " 'future',\n",
       " 'coffee',\n",
       " 'uncertain',\n",
       " 'amount',\n",
       " 'land',\n",
       " 'suitable',\n",
       " 'growing',\n",
       " 'coffee',\n",
       " 'expected',\n",
       " 'shrink',\n",
       " 'estimated',\n",
       " 'according',\n",
       " 'report',\n",
       " 'concept',\n",
       " 'steeped',\n",
       " 'history',\n",
       " 'reveal',\n",
       " 'exactly',\n",
       " 'beanless',\n",
       " 'coffee',\n",
       " 'made',\n",
       " 'company',\n",
       " 'says',\n",
       " 'mixture',\n",
       " 'dozens',\n",
       " 'compounds',\n",
       " 'found',\n",
       " 'food',\n",
       " 'antioxidants',\n",
       " 'flavonoids',\n",
       " 'coffee',\n",
       " 'acids',\n",
       " 'adds',\n",
       " 'caffeine',\n",
       " 'blend',\n",
       " 'slated',\n",
       " 'release',\n",
       " 'first',\n",
       " 'products',\n",
       " 'first',\n",
       " 'brew',\n",
       " 'coffee',\n",
       " 'beans',\n",
       " 'Other',\n",
       " 'startups',\n",
       " 'made',\n",
       " 'popular',\n",
       " 'beverage',\n",
       " 'foods',\n",
       " 'ranging',\n",
       " 'mushrooms',\n",
       " 'acorns',\n",
       " 'failed',\n",
       " 'gain',\n",
       " 'market',\n",
       " 'share',\n",
       " 'chicory',\n",
       " 'proof',\n",
       " 'beanless',\n",
       " 'coffee',\n",
       " 'catch',\n",
       " 'Made',\n",
       " 'roasted',\n",
       " 'ground',\n",
       " 'root',\n",
       " 'namesake',\n",
       " 'plant',\n",
       " 'chicory',\n",
       " 'dates',\n",
       " 'back',\n",
       " '1800s',\n",
       " 'coffee',\n",
       " 'shortages',\n",
       " 'forced',\n",
       " 'people',\n",
       " 'seek',\n",
       " 'substitutes',\n",
       " 'become',\n",
       " 'staple',\n",
       " 'coffee',\n",
       " 'importation',\n",
       " 'became',\n",
       " 'limited',\n",
       " 'turned',\n",
       " 'chicory',\n",
       " 'essentially',\n",
       " 'roasted',\n",
       " 'pieces',\n",
       " 'wood',\n",
       " 'dreamed',\n",
       " 'going',\n",
       " 'deliberately',\n",
       " 'experience',\n",
       " 'coffee',\n",
       " 'really',\n",
       " 'made',\n",
       " 'coffee',\n",
       " 'worked',\n",
       " 'says',\n",
       " 'assistant',\n",
       " 'professor',\n",
       " 'chemistry',\n",
       " 'studies',\n",
       " 'properties',\n",
       " 'coffee',\n",
       " 'immense',\n",
       " 'power',\n",
       " 'able',\n",
       " 're',\n",
       " 'create',\n",
       " 'flavors',\n",
       " 'occur',\n",
       " 'product',\n",
       " 'coffee',\n",
       " 'level',\n",
       " 'almost',\n",
       " 'indiscernible',\n",
       " 'Getting',\n",
       " 'ratios',\n",
       " 'right',\n",
       " 'required',\n",
       " 'drinking',\n",
       " 'spitting',\n",
       " 'several',\n",
       " 'inferior',\n",
       " 'concoctions',\n",
       " 'started',\n",
       " 'wondering',\n",
       " 'science',\n",
       " 'experiment',\n",
       " 'possible',\n",
       " 'something',\n",
       " 'happened',\n",
       " 'early',\n",
       " 'prototypes',\n",
       " 'created',\n",
       " 'garage',\n",
       " 'chlorogenic',\n",
       " 'acid',\n",
       " 'compound',\n",
       " 'contributes',\n",
       " 'bitterness',\n",
       " 'coffee',\n",
       " 'recalls',\n",
       " 'gave',\n",
       " 'cup',\n",
       " 'coffee',\n",
       " 'wife',\n",
       " 'said',\n",
       " 'coffee',\n",
       " 'taste',\n",
       " 'flavor',\n",
       " 'aroma',\n",
       " 'coffee',\n",
       " 'bitterness',\n",
       " 'taste',\n",
       " 'test',\n",
       " 'serves',\n",
       " 'board',\n",
       " 'entrepreneurship',\n",
       " 'program',\n",
       " 'produced',\n",
       " 'rave',\n",
       " 'reviews',\n",
       " 'student',\n",
       " 'tried',\n",
       " 'coffee',\n",
       " 'says',\n",
       " 'like',\n",
       " 'coffee',\n",
       " 'cream',\n",
       " 'sugar',\n",
       " 'tried',\n",
       " 'thought',\n",
       " 'drink',\n",
       " 'black',\n",
       " 'novel',\n",
       " 'really',\n",
       " 'tasty',\n",
       " 'Brewing',\n",
       " 'market',\n",
       " 'alternative',\n",
       " 'coffee',\n",
       " 'taste',\n",
       " 'test',\n",
       " 'method',\n",
       " 'used',\n",
       " 'assess',\n",
       " 'demand',\n",
       " 'campaign',\n",
       " 'raised',\n",
       " 'helped',\n",
       " 'startup',\n",
       " 'presell',\n",
       " 'product',\n",
       " 'draw',\n",
       " 'attention',\n",
       " 'investors',\n",
       " 'completed',\n",
       " 'first',\n",
       " 'round',\n",
       " 'funding',\n",
       " 'total',\n",
       " 'investment',\n",
       " 'released',\n",
       " 'team',\n",
       " 'confident',\n",
       " 'still',\n",
       " 'in',\n",
       " 'development',\n",
       " 'beanless',\n",
       " 'coffee',\n",
       " 'market',\n",
       " 'wanted',\n",
       " 'maintain',\n",
       " 'ritual',\n",
       " 'component',\n",
       " 'coffee',\n",
       " 'waking',\n",
       " 'morning',\n",
       " 'putting',\n",
       " 'grounds',\n",
       " 'coffeemaker',\n",
       " 'wanted',\n",
       " 'replicate',\n",
       " 'scoop',\n",
       " 'scoop',\n",
       " 'says',\n",
       " 'maintained',\n",
       " 'caffeine',\n",
       " 'content',\n",
       " 'fact',\n",
       " 'makes',\n",
       " 'products',\n",
       " 'coffee',\n",
       " 'beans',\n",
       " 'still',\n",
       " 'called',\n",
       " 'coffee',\n",
       " 'standard',\n",
       " 'identity',\n",
       " 'official',\n",
       " 'definition',\n",
       " 'coffee',\n",
       " 'clear',\n",
       " 'coffee',\n",
       " 'come',\n",
       " 'bean',\n",
       " 'fact',\n",
       " 'proud',\n",
       " 'say',\n",
       " 'truth',\n",
       " 'labeling',\n",
       " \"'re\",\n",
       " 'deceiving',\n",
       " 'consumer',\n",
       " 'official',\n",
       " 'regulatory',\n",
       " 'definition',\n",
       " 'still',\n",
       " 'call',\n",
       " 'coffee',\n",
       " 'says',\n",
       " 'sipped',\n",
       " 'countless',\n",
       " 'cups',\n",
       " 'coffee',\n",
       " 'name',\n",
       " 'research',\n",
       " 'eager',\n",
       " 'sample',\n",
       " 'hits',\n",
       " 'market',\n",
       " 'Even',\n",
       " 'smooth',\n",
       " 'cup',\n",
       " 'coffee',\n",
       " 'suspects',\n",
       " 'consumers',\n",
       " 'skeptical',\n",
       " 'adds',\n",
       " 'sincerely',\n",
       " 'hope',\n",
       " 'product',\n",
       " 'excellent',\n",
       " 'figure',\n",
       " 'way',\n",
       " 'navigate',\n",
       " 'difficult',\n",
       " 'space',\n",
       " 'selling',\n",
       " 'concoction',\n",
       " 'compounds',\n",
       " 'perceived',\n",
       " 'similar',\n",
       " 'coffee',\n",
       " 'chemistry',\n",
       " 'definitely',\n",
       " 'interesting',\n",
       " 'journalist',\n",
       " 'beekeeper',\n",
       " 'frequently',\n",
       " 'writes',\n",
       " 'food',\n",
       " 'farming']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#And now I'm going to pull content words: nouns, adjectives, verbs, adverbs. This will be the dictionary\n",
    "#of words from which limerick lines will be formed.\n",
    "def words_to_use(text):\n",
    "    text_tokens = nltk.word_tokenize(text)\n",
    "    text_pos = nltk.pos_tag(text_tokens)\n",
    "    stop_words = list(set(stopwords.words('english')))+[\"'s\",'.','·',\"n't\",'%']\n",
    "    text_words = [(word,pos) for (word, pos) in text_pos if word not in stop_words]\n",
    "    pos_tags = ['NN','NNS','JJ','JJR','JJS','VB','VBD','VBG','VBN','VBP','VBZ','RB','RBR','RBS']\n",
    "    words_to_use = [word for (word,pos) in text_words if pos in pos_tags]\n",
    "    neatened_words_to_use = []\n",
    "    for n in range(0,len(words_to_use)): #to fix issue with hyphenated words\n",
    "        neatened_words_to_use += words_to_use[n].split('-')\n",
    "    return neatened_words_to_use\n",
    "\n",
    "words_to_use(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we're going to build up pronunciation tables with numbers of syllables and where the stresses\n",
    "#So, here I build a function that we can apply to a given word to find out the number of syllables\n",
    "#It's broken into two steps, one to apply to an individual word and one to apply to a tuple like a name\n",
    "def word_pronouncer(word):\n",
    "    pron = pron_dict[word.lower()]\n",
    "    stresses = str()\n",
    "    for phoneme in pron:\n",
    "        if phoneme[-1].isdigit():\n",
    "            #NLTK has markings of 0 for unstressed and 1 and 2 for stressed (primary and secondary)\n",
    "            if phoneme[-1] == '0':\n",
    "                stresses += phoneme[-1]\n",
    "            else:\n",
    "                stresses += '1'\n",
    "    return {'stresses':stresses, 'syllables':len(stresses)}\n",
    "\n",
    "def robust_pronouncer(word):\n",
    "    if type(word) != tuple: #we need to account for the names that will be passed in as tuples\n",
    "        return word_pronouncer(word)\n",
    "    word1 = word_pronouncer(word[0].lower())\n",
    "    word2 = word_pronouncer(word[1].lower())\n",
    "    return {'stresses':word1['stresses']+word2['stresses'],'syllables':word1['syllables']+word2['syllables']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stresses': '110', 'syllables': 3}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "robust_pronouncer('newsworthy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stresses': '10010', 'syllables': 5}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "robust_pronouncer(('Christopher', 'Hendon'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: [('Fire', '1'),\n",
       "  ('New', '1'),\n",
       "  ('North', '1'),\n",
       "  ('Drug', '1'),\n",
       "  ('takes', '1'),\n",
       "  ('first', '1'),\n",
       "  ('sip', '1'),\n",
       "  ('adds', '1'),\n",
       "  ('cream', '1'),\n",
       "  ('mask', '1'),\n",
       "  ('thought', '1'),\n",
       "  ('cup', '1'),\n",
       "  ('joe', '1'),\n",
       "  ('food', '1'),\n",
       "  ('bean', '1'),\n",
       "  ('break', '1'),\n",
       "  ('core', '1'),\n",
       "  ('look', '1'),\n",
       "  ('worked', '1'),\n",
       "  ('food', '1'),\n",
       "  ('brands', '1'),\n",
       "  ('launch', '1'),\n",
       "  ('pair', '1'),\n",
       "  ('turned', '1'),\n",
       "  ('lab', '1'),\n",
       "  ('spent', '1'),\n",
       "  ('months', '1'),\n",
       "  ('green', '1'),\n",
       "  ('beans', '1'),\n",
       "  ('beans', '1'),\n",
       "  ('brewed', '1'),\n",
       "  ('gas', '1'),\n",
       "  ('got', '1'),\n",
       "  ('threats', '1'),\n",
       "  ('world', '1'),\n",
       "  ('whole', '1'),\n",
       "  ('threats', '1'),\n",
       "  ('called', '1'),\n",
       "  ('rust', '1'),\n",
       "  ('great', '1'),\n",
       "  ('says', '1'),\n",
       "  ('land', '1'),\n",
       "  ('shrink', '1'),\n",
       "  ('steeped', '1'),\n",
       "  ('made', '1'),\n",
       "  ('says', '1'),\n",
       "  ('found', '1'),\n",
       "  ('food', '1'),\n",
       "  ('adds', '1'),\n",
       "  ('blend', '1'),\n",
       "  ('first', '1'),\n",
       "  ('first', '1'),\n",
       "  ('brew', '1'),\n",
       "  ('beans', '1'),\n",
       "  ('made', '1'),\n",
       "  ('foods', '1'),\n",
       "  ('failed', '1'),\n",
       "  ('gain', '1'),\n",
       "  ('share', '1'),\n",
       "  ('proof', '1'),\n",
       "  ('catch', '1'),\n",
       "  ('Made', '1'),\n",
       "  ('ground', '1'),\n",
       "  ('root', '1'),\n",
       "  ('plant', '1'),\n",
       "  ('dates', '1'),\n",
       "  ('back', '1'),\n",
       "  ('forced', '1'),\n",
       "  ('seek', '1'),\n",
       "  ('turned', '1'),\n",
       "  ('wood', '1'),\n",
       "  ('dreamed', '1'),\n",
       "  ('made', '1'),\n",
       "  ('worked', '1'),\n",
       "  ('says', '1'),\n",
       "  ('re', '1'),\n",
       "  ('right', '1'),\n",
       "  ('gave', '1'),\n",
       "  ('cup', '1'),\n",
       "  ('wife', '1'),\n",
       "  ('said', '1'),\n",
       "  ('taste', '1'),\n",
       "  ('taste', '1'),\n",
       "  ('test', '1'),\n",
       "  ('serves', '1'),\n",
       "  ('board', '1'),\n",
       "  ('rave', '1'),\n",
       "  ('tried', '1'),\n",
       "  ('says', '1'),\n",
       "  ('like', '1'),\n",
       "  ('cream', '1'),\n",
       "  ('tried', '1'),\n",
       "  ('thought', '1'),\n",
       "  ('drink', '1'),\n",
       "  ('black', '1'),\n",
       "  ('taste', '1'),\n",
       "  ('test', '1'),\n",
       "  ('used', '1'),\n",
       "  ('raised', '1'),\n",
       "  ('helped', '1'),\n",
       "  ('draw', '1'),\n",
       "  ('first', '1'),\n",
       "  ('round', '1'),\n",
       "  ('team', '1'),\n",
       "  ('still', '1'),\n",
       "  ('in', '1'),\n",
       "  ('grounds', '1'),\n",
       "  ('scoop', '1'),\n",
       "  ('scoop', '1'),\n",
       "  ('says', '1'),\n",
       "  ('fact', '1'),\n",
       "  ('makes', '1'),\n",
       "  ('beans', '1'),\n",
       "  ('still', '1'),\n",
       "  ('called', '1'),\n",
       "  ('clear', '1'),\n",
       "  ('come', '1'),\n",
       "  ('bean', '1'),\n",
       "  ('fact', '1'),\n",
       "  ('proud', '1'),\n",
       "  ('say', '1'),\n",
       "  ('truth', '1'),\n",
       "  ('still', '1'),\n",
       "  ('call', '1'),\n",
       "  ('says', '1'),\n",
       "  ('sipped', '1'),\n",
       "  ('cups', '1'),\n",
       "  ('name', '1'),\n",
       "  ('hits', '1'),\n",
       "  ('smooth', '1'),\n",
       "  ('cup', '1'),\n",
       "  ('adds', '1'),\n",
       "  ('hope', '1'),\n",
       "  ('way', '1'),\n",
       "  ('space', '1'),\n",
       "  ('writes', '1'),\n",
       "  ('food', '1')],\n",
       " 2: [('Kettle', '10'),\n",
       "  ('Taylor', '10'),\n",
       "  ('July', '01'),\n",
       "  ('Never', '10'),\n",
       "  ('Helmer', '10'),\n",
       "  ('Center', '10'),\n",
       "  ('coffee', '10'),\n",
       "  ('sugar', '10'),\n",
       "  ('settle', '10'),\n",
       "  ('coffee', '10'),\n",
       "  ('brewing', '10'),\n",
       "  ('started', '10'),\n",
       "  ('thinking', '10'),\n",
       "  ('able', '10'),\n",
       "  ('coffee', '10'),\n",
       "  ('explains', '01'),\n",
       "  ('garage', '01'),\n",
       "  ('brewing', '10'),\n",
       "  ('running', '10'),\n",
       "  ('roasted', '10'),\n",
       "  ('coffee', '10'),\n",
       "  ('liquid', '10'),\n",
       "  ('separate', '10'),\n",
       "  ('compounds', '01'),\n",
       "  ('coffee', '10'),\n",
       "  ('create', '01'),\n",
       "  ('product', '10'),\n",
       "  ('color', '10'),\n",
       "  ('flavor', '10'),\n",
       "  ('coffee', '10'),\n",
       "  ('deeper', '10'),\n",
       "  ('process', '11'),\n",
       "  ('learned', '10'),\n",
       "  ('coffee', '10'),\n",
       "  ('global', '10'),\n",
       "  ('warming', '10'),\n",
       "  ('fungus', '10'),\n",
       "  ('even', '10'),\n",
       "  ('making', '10'),\n",
       "  ('coffee', '10'),\n",
       "  ('also', '10'),\n",
       "  ('better', '10'),\n",
       "  ('future', '10'),\n",
       "  ('coffee', '10'),\n",
       "  ('amount', '01'),\n",
       "  ('growing', '10'),\n",
       "  ('coffee', '10'),\n",
       "  ('report', '01'),\n",
       "  ('concept', '10'),\n",
       "  ('history', '10'),\n",
       "  ('reveal', '01'),\n",
       "  ('coffee', '10'),\n",
       "  ('mixture', '10'),\n",
       "  ('dozens', '10'),\n",
       "  ('compounds', '01'),\n",
       "  ('coffee', '10'),\n",
       "  ('acids', '10'),\n",
       "  ('caffeine', '01'),\n",
       "  ('slated', '10'),\n",
       "  ('release', '01'),\n",
       "  ('products', '10'),\n",
       "  ('coffee', '10'),\n",
       "  ('Other', '10'),\n",
       "  ('startups', '11'),\n",
       "  ('beverage', '10'),\n",
       "  ('ranging', '10'),\n",
       "  ('mushrooms', '10'),\n",
       "  ('acorns', '10'),\n",
       "  ('market', '10'),\n",
       "  ('coffee', '10'),\n",
       "  ('roasted', '10'),\n",
       "  ('namesake', '11'),\n",
       "  ('coffee', '10'),\n",
       "  ('people', '10'),\n",
       "  ('become', '01'),\n",
       "  ('staple', '10'),\n",
       "  ('coffee', '10'),\n",
       "  ('became', '01'),\n",
       "  ('roasted', '10'),\n",
       "  ('pieces', '10'),\n",
       "  ('going', '10'),\n",
       "  ('coffee', '10'),\n",
       "  ('really', '10'),\n",
       "  ('coffee', '10'),\n",
       "  ('studies', '10'),\n",
       "  ('coffee', '10'),\n",
       "  ('immense', '01'),\n",
       "  ('power', '10'),\n",
       "  ('able', '10'),\n",
       "  ('create', '01'),\n",
       "  ('flavors', '10'),\n",
       "  ('occur', '01'),\n",
       "  ('product', '10'),\n",
       "  ('coffee', '10'),\n",
       "  ('level', '10'),\n",
       "  ('almost', '11'),\n",
       "  ('Getting', '10'),\n",
       "  ('required', '01'),\n",
       "  ('drinking', '10'),\n",
       "  ('spitting', '10'),\n",
       "  ('started', '10'),\n",
       "  ('science', '10'),\n",
       "  ('something', '10'),\n",
       "  ('happened', '10'),\n",
       "  ('early', '10'),\n",
       "  ('garage', '01'),\n",
       "  ('acid', '10'),\n",
       "  ('compound', '01'),\n",
       "  ('coffee', '10'),\n",
       "  ('recalls', '01'),\n",
       "  ('coffee', '10'),\n",
       "  ('coffee', '10'),\n",
       "  ('flavor', '10'),\n",
       "  ('coffee', '10'),\n",
       "  ('program', '11'),\n",
       "  ('produced', '01'),\n",
       "  ('reviews', '01'),\n",
       "  ('student', '10'),\n",
       "  ('coffee', '10'),\n",
       "  ('coffee', '10'),\n",
       "  ('sugar', '10'),\n",
       "  ('novel', '10'),\n",
       "  ('really', '10'),\n",
       "  ('tasty', '10'),\n",
       "  ('Brewing', '10'),\n",
       "  ('market', '10'),\n",
       "  ('coffee', '10'),\n",
       "  ('method', '10'),\n",
       "  ('assess', '01'),\n",
       "  ('demand', '01'),\n",
       "  ('campaign', '01'),\n",
       "  ('startup', '11'),\n",
       "  ('product', '10'),\n",
       "  ('funding', '10'),\n",
       "  ('total', '10'),\n",
       "  ('released', '01'),\n",
       "  ('coffee', '10'),\n",
       "  ('market', '10'),\n",
       "  ('wanted', '10'),\n",
       "  ('maintain', '01'),\n",
       "  ('coffee', '10'),\n",
       "  ('waking', '10'),\n",
       "  ('morning', '10'),\n",
       "  ('putting', '10'),\n",
       "  ('wanted', '10'),\n",
       "  ('maintained', '01'),\n",
       "  ('caffeine', '01'),\n",
       "  ('content', '01'),\n",
       "  ('products', '10'),\n",
       "  ('coffee', '10'),\n",
       "  ('coffee', '10'),\n",
       "  ('standard', '10'),\n",
       "  ('coffee', '10'),\n",
       "  ('coffee', '10'),\n",
       "  ('labeling', '10'),\n",
       "  ('coffee', '10'),\n",
       "  ('countless', '10'),\n",
       "  ('coffee', '10'),\n",
       "  ('research', '10'),\n",
       "  ('eager', '10'),\n",
       "  ('sample', '10'),\n",
       "  ('market', '10'),\n",
       "  ('Even', '10'),\n",
       "  ('coffee', '10'),\n",
       "  ('suspects', '11'),\n",
       "  ('product', '10'),\n",
       "  ('figure', '10'),\n",
       "  ('selling', '10'),\n",
       "  ('compounds', '01'),\n",
       "  ('perceived', '01'),\n",
       "  ('coffee', '10'),\n",
       "  ('farming', '10')],\n",
       " 5: [('University', '10100'),\n",
       "  ('NPR.org', '11111'),\n",
       "  (('North', 'Carolina'), '11010'),\n",
       "  (('Christopher', 'Hendon'), '10010'),\n",
       "  ('chromatography', '00100'),\n",
       "  ('deforestation', '01010'),\n",
       "  ('antioxidants', '10100'),\n",
       "  ('entrepreneurship', '10010'),\n",
       "  ('regulatory', '10010')],\n",
       " 3: [('Washington', '100'),\n",
       "  ('Tropical', '100'),\n",
       "  ('Graduate', '101'),\n",
       "  ('Oregon', '101'),\n",
       "  (('Taylor', 'Moore'), '101'),\n",
       "  (('New', 'Orleans'), '110'),\n",
       "  ('bitterness', '100'),\n",
       "  ('regular', '100'),\n",
       "  ('scientist', '100'),\n",
       "  ('decided', '010'),\n",
       "  ('bitterness', '100'),\n",
       "  ('components', '010'),\n",
       "  ('optimize', '101'),\n",
       "  ('Seattle', '010'),\n",
       "  ('catalog', '100'),\n",
       "  ('aroma', '010'),\n",
       "  ('committed', '010'),\n",
       "  ('uncertain', '010'),\n",
       "  ('suitable', '100'),\n",
       "  ('expected', '010'),\n",
       "  ('according', '010'),\n",
       "  ('exactly', '010'),\n",
       "  ('company', '100'),\n",
       "  ('popular', '100'),\n",
       "  ('shortages', '100'),\n",
       "  ('substitutes', '101'),\n",
       "  ('limited', '100'),\n",
       "  ('assistant', '010'),\n",
       "  ('professor', '010'),\n",
       "  ('chemistry', '100'),\n",
       "  ('properties', '100'),\n",
       "  ('ratios', '101'),\n",
       "  ('several', '100'),\n",
       "  ('concoctions', '010'),\n",
       "  ('wondering', '100'),\n",
       "  ('possible', '100'),\n",
       "  ('prototypes', '101'),\n",
       "  ('created', '010'),\n",
       "  ('contributes', '010'),\n",
       "  ('bitterness', '100'),\n",
       "  ('aroma', '010'),\n",
       "  ('bitterness', '100'),\n",
       "  ('attention', '010'),\n",
       "  ('investors', '010'),\n",
       "  ('completed', '010'),\n",
       "  ('investment', '010'),\n",
       "  ('confident', '100'),\n",
       "  ('ritual', '100'),\n",
       "  ('component', '010'),\n",
       "  ('replicate', '101'),\n",
       "  ('official', '010'),\n",
       "  ('deceiving', '010'),\n",
       "  ('consumer', '010'),\n",
       "  ('official', '010'),\n",
       "  ('consumers', '010'),\n",
       "  ('skeptical', '100'),\n",
       "  ('sincerely', '010'),\n",
       "  ('excellent', '100'),\n",
       "  ('navigate', '101'),\n",
       "  ('difficult', '100'),\n",
       "  ('concoction', '010'),\n",
       "  ('similar', '100'),\n",
       "  ('chemistry', '100'),\n",
       "  ('journalist', '100'),\n",
       "  ('beekeeper', '110'),\n",
       "  ('frequently', '100')],\n",
       " 4: [(('Jodi', 'Helmer'), '1010'),\n",
       "  ('reengineer', '0101'),\n",
       "  ('entrepreneur', '1001'),\n",
       "  ('environment', '0100'),\n",
       "  ('devastating', '1010'),\n",
       "  ('consistently', '0100'),\n",
       "  ('environment', '0100'),\n",
       "  ('estimated', '1010'),\n",
       "  ('importation', '1010'),\n",
       "  ('essentially', '0100'),\n",
       "  ('deliberately', '0100'),\n",
       "  ('experience', '0100'),\n",
       "  ('inferior', '0100'),\n",
       "  ('experiment', '0100'),\n",
       "  ('alternative', '0100'),\n",
       "  ('development', '0100'),\n",
       "  ('identity', '0100'),\n",
       "  ('definition', '1010'),\n",
       "  ('definition', '1010'),\n",
       "  ('definitely', '1000'),\n",
       "  ('interesting', '1000')],\n",
       " 7: [(('Tropical', 'Agriculture'), '1001010'),\n",
       "  (('International', 'Center'), '1010010')],\n",
       " 6: [(('Drug', 'Administration'), '101010')]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#And now I will create a master dictionary to call from with all the words and their stresses, \n",
    "#organized by syllable counts\n",
    "def master_dict_maker(text):\n",
    "    list_of_words = find_proper_nouns(text) + words_to_use(text)\n",
    "    master_dict = {}\n",
    "    for word in list_of_words:\n",
    "        try:\n",
    "            pron = robust_pronouncer(word)\n",
    "            if pron['syllables'] in master_dict.keys():\n",
    "                master_dict[pron['syllables']].append((word, pron['stresses']))\n",
    "            else:\n",
    "                master_dict[pron['syllables']] = [(word, pron['stresses'])]\n",
    "        except:\n",
    "            pass\n",
    "    return master_dict\n",
    "\n",
    "master_dict = master_dict_maker(text)\n",
    "master_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so here's the order of operations as I imagine it:\n",
    "1. Find a topic, find the rhyme-set from the overall text\n",
    "2. Decide which words are going to be the line ending rhymes: If the topic occurs within the rhyme set, use that, else we'll slot the topic in elsewhere\n",
    "3. Create the master dictionary of words by syllable count and stresses\n",
    "4. Build lines by pulling from from the master dictionary\n",
    "5. Check how many syllables are left to assign in a line, pull all the words from the master dictionary that are shorter than that\n",
    "6. Pull a random word from this sub-list and check if it fits into the stress pattern, if it does, slot it in, else pull another random word until you find one that works\n",
    "7. Finalize your non-sense line\n",
    "\n",
    "I'm going to use a loose stress pattern requirement: every third syllable stressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('possible', 'able', 'suitable'), ('become', 'come'))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here's a function to find all the rhyming sets, and pick two semi-randomly to be the a and b rhymes\n",
    "def assign_rhymes(text):\n",
    "    rhyme_dict = find_rhymes(text)\n",
    "    rhyme_list = list(rhyme_dict.values())\n",
    "    rhyme_list_3 = [rhyme for rhyme in rhyme_list if len(rhyme)>2]\n",
    "    a_rhyme = rhyme_list_3[np.random.randint(1,len(rhyme_list_3))]\n",
    "    searching = True\n",
    "    while searching == True: #Putting check in here so we don't pick the same rhyme set for a and b\n",
    "        b_rhyme = rhyme_list[np.random.randint(1,len(rhyme_list_3))]\n",
    "        if b_rhyme != a_rhyme:\n",
    "            searching = False\n",
    "    a_in = np.random.choice(len(a_rhyme), 3, replace=False) #sample 3 words out of the rhyme list\n",
    "    b_in = np.random.choice(len(b_rhyme), 2, replace=False)\n",
    "    return (a_rhyme[a_in[0]],a_rhyme[a_in[1]],a_rhyme[a_in[2]]), (b_rhyme[b_in[0]],b_rhyme[b_in[1]])\n",
    "\n",
    "assign_rhymes(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here's a function that picks out a word to use for a line, such that it is withing a given syllable count\n",
    "# and that it has stressed accents at the right places\n",
    "def find_word_for_line(max_syllables, master_dict, stress_pattern):\n",
    "    word_list = []\n",
    "    for n in master_dict.keys():\n",
    "        if n <= max_syllables:\n",
    "            word_list = word_list + master_dict[n]\n",
    "    searching = True\n",
    "    while searching == True:\n",
    "        word = word_list[np.random.randint(1,len(word_list)+1)]\n",
    "        stress_test = 0\n",
    "        for i in range(0,len(word[1])):\n",
    "            if stress_pattern[i] == '1':\n",
    "                if word[1][i] != stress_pattern[i]:\n",
    "                    stress_test +=1\n",
    "        if stress_test == 0:\n",
    "            searching = False\n",
    "    return word\n",
    "\n",
    "# and here's the function to fill in a line given the word dictionary, number of remaining syllables, stress pattern\n",
    "# it will take in a line that's partially filled\n",
    "def fill_out_line(syllables_total, master_dict, stress_pattern, line_beginning=[]):\n",
    "    line = [word for word in line_beginning]\n",
    "    syllables_left = syllables_total\n",
    "    stress_pattern = stress_pattern\n",
    "    while syllables_left >0:\n",
    "        word = find_word_for_line(syllables_left, master_dict, stress_pattern)\n",
    "        line.append(word[0])\n",
    "        syllables_left -= len(word[1])\n",
    "        stress_pattern = stress_pattern[len(word[1])-1:]\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now to actually build out our limerick:\n",
    "def write_nonsense_limerick_simple(text):\n",
    "    a_rhymes, b_rhymes = assign_rhymes(text)\n",
    "    master_dict = master_dict_maker(text)\n",
    "    a_1 = (a_rhymes[0],robust_pronouncer(a_rhymes[0])['syllables'])\n",
    "    line_1 = fill_out_line(8-a_1[1], master_dict, '10010010'[:-a_1[1]])+[a_1[0]]\n",
    "    a_2 = (a_rhymes[1],robust_pronouncer(a_rhymes[1])['syllables'])\n",
    "    line_2 = fill_out_line(8-a_2[1], master_dict, '10010010'[:-a_2[1]])+[a_2[0]]\n",
    "    b_1 = (b_rhymes[0],robust_pronouncer(b_rhymes[0])['syllables'])\n",
    "    line_3 = fill_out_line(5-b_1[1], master_dict, '10010010'[:-b_1[1]])+[b_1[0]]\n",
    "    b_2 = (b_rhymes[1],robust_pronouncer(b_rhymes[1])['syllables'])\n",
    "    line_4 = fill_out_line(5-b_2[1], master_dict, '10010010'[:-b_2[1]])+[b_2[0]]\n",
    "    a_3 = (a_rhymes[2],robust_pronouncer(a_rhymes[2])['syllables'])\n",
    "    line_5 = fill_out_line(8-a_3[1], master_dict, '10010010'[:-a_3[1]])+[a_3[0]]\n",
    "    return line_1, line_2,line_3,line_4,line_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['suspects', 'New', 'coffee', 'food', 'writes', 'ground'],\n",
       " ['forced', 'drinking', 'devastating', 'found'],\n",
       " ['beverage', 'share', 'waking'],\n",
       " ['proud', 'brew', 'thought', 'making'],\n",
       " ['wanted', 'coffee', 'like', 'bean', 'compound'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_nonsense_limerick_simple(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the article is about a non-coffee bean based coffe like alternative, this last line is shockingly apt! And we built this from randomly pulled words. Says something about the flexibility of language and how suggestible we are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I'm going to incorporate the text parsing. First step is simply to just to parse the text into the nouns, chunks, adjectives from your function. Then I'll tweak my functions to work with everything split out. Steps are:\n",
    "\n",
    "1. find rhymes from among nouns and noun chunks or among verbs - the things that can come at the end of a given line as the 'objects' or as verbs in simpler sentences (it occurs to me to make the short lines end with verbs)\n",
    "2. create pronunciation dictionaries for each part of speech - have these as seperate dictionaries\n",
    "3. create a new line creator function that will take into account the part of speech - start with a noun chunk, add a verb end with the object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word parsing tweaked somewhat from my partner Jen McKaig's work\n",
    "def word_parse(x):\n",
    "    text = nlp(str(x)) \n",
    "    tokens = [token for token in text if not token.is_stop]\n",
    "    res = {\n",
    "            'nouns':[],\n",
    "            'verbs':[],\n",
    "            'adjectives':[],\n",
    "            'pronouns':[],\n",
    "            'adverbs':[],\n",
    "            'noun_chunks':[]\n",
    "        }\n",
    "    for token in tokens:\n",
    "        if token.pos_ == 'NOUN':\n",
    "            res['nouns'].append(token)\n",
    "        elif token.pos_ == 'VERB':\n",
    "            res['verbs'].append(token)\n",
    "        elif token.pos_ == 'ADJ':\n",
    "            res['adjectives'].append(token)\n",
    "        elif token.pos_ == 'PRON':\n",
    "            res['pronouns'].append(token)\n",
    "        elif token.pos_ == 'ADV':\n",
    "            res['adverbs'].append(token)\n",
    "            \n",
    "    for chunk in text.noun_chunks:\n",
    "            res['noun_chunks'].append(chunk)\n",
    "    res['noun_chunks'] = list(set(res['noun_chunks']))\n",
    "            \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[sip,\n",
       " coffee,\n",
       " cream,\n",
       " sugar,\n",
       " bitterness,\n",
       " cup,\n",
       " joe,\n",
       " food,\n",
       " scientist,\n",
       " reengineer,\n",
       " coffee,\n",
       " bitterness,\n",
       " bean,\n",
       " coffee,\n",
       " core,\n",
       " components,\n",
       " food,\n",
       " brands,\n",
       " entrepreneur,\n",
       " pair,\n",
       " garage,\n",
       " brewing,\n",
       " lab,\n",
       " months,\n",
       " beans,\n",
       " beans,\n",
       " coffee,\n",
       " gas,\n",
       " chromatography,\n",
       " compounds,\n",
       " coffee,\n",
       " product,\n",
       " color,\n",
       " aroma,\n",
       " flavor,\n",
       " mouthfeel,\n",
       " coffee,\n",
       " process,\n",
       " threats,\n",
       " coffee,\n",
       " world,\n",
       " threats,\n",
       " environment,\n",
       " deforestation,\n",
       " warming,\n",
       " fungus,\n",
       " rust,\n",
       " coffee,\n",
       " environment,\n",
       " future,\n",
       " coffee,\n",
       " land,\n",
       " coffee,\n",
       " %,\n",
       " report,\n",
       " concept,\n",
       " history,\n",
       " beanless,\n",
       " coffee,\n",
       " company,\n",
       " mixture,\n",
       " dozens,\n",
       " compounds,\n",
       " food,\n",
       " antioxidants,\n",
       " flavonoids,\n",
       " coffee,\n",
       " acids,\n",
       " caffeine,\n",
       " blend,\n",
       " products,\n",
       " coffee,\n",
       " beans,\n",
       " startups,\n",
       " beverage,\n",
       " foods,\n",
       " mushrooms,\n",
       " acorns,\n",
       " market,\n",
       " share,\n",
       " chicory,\n",
       " proof,\n",
       " beanless,\n",
       " coffee,\n",
       " ground,\n",
       " root,\n",
       " namesake,\n",
       " plant,\n",
       " chicory,\n",
       " 1800s,\n",
       " coffee,\n",
       " shortages,\n",
       " people,\n",
       " substitutes,\n",
       " staple,\n",
       " coffee,\n",
       " importation,\n",
       " chicory,\n",
       " pieces,\n",
       " wood,\n",
       " coffee,\n",
       " coffee,\n",
       " professor,\n",
       " chemistry,\n",
       " properties,\n",
       " coffee,\n",
       " power,\n",
       " flavors,\n",
       " product,\n",
       " coffee,\n",
       " level,\n",
       " ratios,\n",
       " drinking,\n",
       " concoctions,\n",
       " science,\n",
       " experiment,\n",
       " prototypes,\n",
       " garage,\n",
       " acid,\n",
       " compound,\n",
       " bitterness,\n",
       " coffee,\n",
       " cup,\n",
       " coffee,\n",
       " wife,\n",
       " coffee,\n",
       " flavor,\n",
       " aroma,\n",
       " coffee,\n",
       " bitterness,\n",
       " taste,\n",
       " test,\n",
       " board,\n",
       " entrepreneurship,\n",
       " program,\n",
       " rave,\n",
       " reviews,\n",
       " student,\n",
       " coffee,\n",
       " coffee,\n",
       " cream,\n",
       " sugar,\n",
       " black,\n",
       " market,\n",
       " coffee,\n",
       " taste,\n",
       " test,\n",
       " method,\n",
       " demand,\n",
       " campaign,\n",
       " startup,\n",
       " product,\n",
       " attention,\n",
       " investors,\n",
       " round,\n",
       " funding,\n",
       " investment,\n",
       " team,\n",
       " development,\n",
       " beanless,\n",
       " coffee,\n",
       " market,\n",
       " component,\n",
       " coffee,\n",
       " morning,\n",
       " grounds,\n",
       " coffeemaker,\n",
       " scoop,\n",
       " scoop,\n",
       " caffeine,\n",
       " content,\n",
       " fact,\n",
       " products,\n",
       " coffee,\n",
       " beans,\n",
       " coffee,\n",
       " standard,\n",
       " identity,\n",
       " definition,\n",
       " coffee,\n",
       " coffee,\n",
       " bean,\n",
       " fact,\n",
       " truth,\n",
       " labeling,\n",
       " consumer,\n",
       " definition,\n",
       " coffee,\n",
       " cups,\n",
       " coffee,\n",
       " research,\n",
       " market,\n",
       " cup,\n",
       " coffee,\n",
       " consumers,\n",
       " product,\n",
       " way,\n",
       " space,\n",
       " concoction,\n",
       " compounds,\n",
       " coffee,\n",
       " chemistry,\n",
       " journalist,\n",
       " beekeeper,\n",
       " food,\n",
       " farming]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_text = word_parse(text)\n",
    "parsed_text['nouns']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(parsed_text['noun_chunks'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we build out rhymes. Rhymes from noun chunks and rhymes from verbs kept seperately\n",
    "# need to turn multi word noun chunks into tuples or lists of text\n",
    "def create_text(token):\n",
    "    stop_words = list(set(stopwords.words('english'))) + ['\"']\n",
    "    if len(token) == 1:\n",
    "        if token.text.lower() not in stop_words:\n",
    "            return token.text.lower()\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return [sub.text.lower() for sub in token if sub.text.lower() not in ['\"',',',\"'\",'-']]\n",
    "\n",
    "\n",
    "def rhyme_from_list(pronunciations):\n",
    "    match_dict = {}\n",
    "    for n in range(0,len(pronunciations)): #given one word in the list\n",
    "        for i in range(n+1,len(pronunciations)): #we'll check it against all words that follow\n",
    "            if (pronunciations[n][1][1:] == pronunciations[i][1][-len(pronunciations[n][1][1:]):] or\n",
    "                    pronunciations[i][1][1:] == pronunciations[n][1][-len(pronunciations[i][1][1:]):]):\n",
    "                # need to cutt out super short rhymes - else you get things like 'is' rhyming\n",
    "                # with every word that ends with s\n",
    "                if ((len(pronunciations[n][1][1:]) > 2 or len(pronunciations[n][1]) ==3) and \n",
    "                    (len(pronunciations[i][1][1:]) > 2 or len(pronunciations[i][1]) ==2)):\n",
    "                    if len(pronunciations[n][1][1:]) < len(pronunciations[i][1][1:]): \n",
    "                        rhyme = ''.join(pronunciations[n][1][1:])\n",
    "                    else:\n",
    "                        rhyme = ''.join(pronunciations[i][1][1:])\n",
    "                    if len(rhyme) > 2:\n",
    "                    #if this rhyme has already been added to the dictionary of rhymes we can include it\n",
    "                        if rhyme in match_dict.keys(): \n",
    "                            match_dict[rhyme] = list(set(match_dict[rhyme]+\n",
    "                                                         [pronunciations[n][0],pronunciations[i][0]]))\n",
    "                        else: #if the dictionary doesn't have an entry for this rhyme, we put in a \n",
    "                            match_dict[rhyme] = [pronunciations[n][0],pronunciations[i][0]]\n",
    "\n",
    "    return match_dict\n",
    "    match_dict = {}\n",
    "    \n",
    "    \n",
    "def parsed_rhymes(parsed_text):\n",
    "    stop_words = list(set(stopwords.words('english')))\n",
    "    nouns = [] #minor snag taking the set of a list with nested lists...\n",
    "    for item in parsed_text['nouns']:\n",
    "        nouns.append(item.text.lower())\n",
    "    noun_list = list(set(nouns))\n",
    "\n",
    "    #building simple text lists to iterate rhyme finding function over\n",
    "    verb_list = [token.text.lower() for token in parsed_text['verbs']]\n",
    "    verb_list = [item for item in set(verb_list) if item not in stop_words]\n",
    "    #attaching pronunciations \n",
    "    noun_pron = []\n",
    "    for word in noun_list:\n",
    "        if type(word) == str:\n",
    "            try:\n",
    "                noun_pron.append((word,pron_dict[word]))\n",
    "            except:\n",
    "                pass\n",
    "        else:\n",
    "            try:\n",
    "                noun_pron.append((word,pron_dict[word[-1]]))\n",
    "            except:\n",
    "                pass\n",
    "    noun_rhymes = rhyme_from_list(noun_pron)\n",
    "    \n",
    "    verb_pron = []\n",
    "    for word in verb_list:\n",
    "        if type(word) == str:\n",
    "            try:\n",
    "                verb_pron.append((word,pron_dict[word]))\n",
    "            except:\n",
    "                pass\n",
    "    verb_rhymes = rhyme_from_list(verb_pron)\n",
    "    master_dict = {'nouns':noun_rhymes, 'verbs':verb_rhymes}\n",
    "    return master_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nouns': {'AW1ND': ['ground', 'compound', 'round'],\n",
       "  'AE1ND': ['demand', 'land']},\n",
       " 'verbs': {'RIH1NGK': ['shrink', 'drink'],\n",
       "  'EY1KS': ['makes', 'takes'],\n",
       "  'EY1KIH0NG': ['making', 'waking'],\n",
       "  'IH1TIH0NG': ['getting', 'spitting']}}"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_rhymes(parsed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can assign our rhyme scheme, using nouns for the a rhymes and verbs for the b rhymes.\n",
    "\n",
    "I'll also adjust my pronunciation functions to pass over the dictionary of parsed words/chunks and return a dictionary that is still partitioned by part of speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_pos_rhymes(parsed_text):\n",
    "    rhyme_dict = parsed_rhymes(parsed_text)\n",
    "    noun_rhyme_list = list(rhyme_dict['nouns'].values())\n",
    "    rhyme_list_3 = [rhyme for rhyme in noun_rhyme_list if len(rhyme)>2]\n",
    "    if len(rhyme_list_3) >1:\n",
    "        a_rhyme = rhyme_list_3[np.random.randint(1,len(rhyme_list_3))]\n",
    "    else:\n",
    "        a_rhyme = rhyme_list_3[0]\n",
    "    verb_rhyme_list = list(rhyme_dict['verbs'].values())\n",
    "    if len(verb_rhyme_list) > 1:\n",
    "        b_rhyme = verb_rhyme_list[np.random.randint(1,len(verb_rhyme_list))]\n",
    "    else:\n",
    "        b_rhyme = verb_rhyme_list[0]\n",
    "    \n",
    "    a_in = np.random.choice(len(a_rhyme), 3, replace=False) #sample 3 words out of the rhyme list\n",
    "    b_in = np.random.choice(len(b_rhyme), 2, replace=False)\n",
    "    return (a_rhyme[a_in[0]],a_rhyme[a_in[1]],a_rhyme[a_in[2]]), (b_rhyme[b_in[0]],b_rhyme[b_in[1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('compound', 'round', 'ground'), ('makes', 'takes'))"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assign_pos_rhymes(parsed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_pronouncer_pos(word):\n",
    "    if type(word) != spacy.tokens.span.Span: \n",
    "        return word_pronouncer(word.text.lower())\n",
    "    else:\n",
    "        temp = []\n",
    "        for n in range(0,len(word)):\n",
    "            temp.append(word_pronouncer(word[n].text.lower()))\n",
    "        stresses = str()\n",
    "        for item in temp:\n",
    "            stresses += item['stresses']\n",
    "    return {'stresses':stresses,'syllables':len(stresses)}\n",
    "\n",
    "def master_dict_maker_for_parsed(list_of_words):\n",
    "    master_dict = {}\n",
    "    for word in list_of_words:\n",
    "        try:\n",
    "            pron = robust_pronouncer_pos(word)\n",
    "            if pron['syllables'] in master_dict.keys():\n",
    "                master_dict[pron['syllables']].append((word, pron['stresses']))\n",
    "            else:\n",
    "                master_dict[pron['syllables']] = [(word, pron['stresses'])]\n",
    "        except:\n",
    "            pass\n",
    "    return master_dict\n",
    "#returning a slightly different sort of master dictionary, now it's nested, \n",
    "#with sub-dictionaries for each part of speach\n",
    "def parsed_pronunciation_dict(parsed_text):\n",
    "    pron_dict = {}\n",
    "    pron_dict['nouns'] = master_dict_maker_for_parsed(parsed_text['nouns'])\n",
    "    pron_dict['verbs'] = master_dict_maker_for_parsed(parsed_text['verbs'])\n",
    "    pron_dict['adjectives'] = master_dict_maker_for_parsed(parsed_text['adjectives'])\n",
    "    pron_dict['noun_chunks'] = master_dict_maker_for_parsed(parsed_text['noun_chunks'])\n",
    "    return pron_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_dict = parsed_pronunciation_dict(parsed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{4: [(roasted pieces, '1010'),\n",
       "  (the bitterness, '0100'),\n",
       "  (global warming, '1010'),\n",
       "  (the consumer, '0010'),\n",
       "  (coffee acids, '1010'),\n",
       "  (The chemistry, '0100'),\n",
       "  (its namesake plant, '0111'),\n",
       "  (Jodi Helmer, '1010'),\n",
       "  (the bitterness, '0100'),\n",
       "  (a brewing lab, '1101'),\n",
       "  (its first products, '0110'),\n",
       "  (the same color, '0110'),\n",
       "  (Other startups, '1011'),\n",
       "  (the attention, '0010'),\n",
       "  (development, '0100'),\n",
       "  (required drinking, '0110'),\n",
       "  (the company, '0100'),\n",
       "  (the bitterness, '0100'),\n",
       "  (that bitterness, '0100'),\n",
       "  (a concoction, '1010'),\n",
       "  (exactly what, '0101'),\n",
       "  (the coffee world, '0101'),\n",
       "  (the properties, '0100'),\n",
       "  (growing coffee, '1010'),\n",
       "  (identity, '0100'),\n",
       "  (the ratios, '0101'),\n",
       "  (other food brands, '1011')],\n",
       " 3: [(a smooth cup, '111'),\n",
       "  (its product, '010'),\n",
       "  (the market, '010'),\n",
       "  (our coffee, '110'),\n",
       "  (market share, '101'),\n",
       "  (rave reviews, '101'),\n",
       "  (A concept, '110'),\n",
       "  (my coffee, '110'),\n",
       "  (the process, '011'),\n",
       "  (consumers, '010'),\n",
       "  (substitutes, '101'),\n",
       "  (chemistry, '100'),\n",
       "  (its products, '010'),\n",
       "  (its first round, '011'),\n",
       "  (a mixture, '110'),\n",
       "  (The taste test, '011'),\n",
       "  (the flavor, '010'),\n",
       "  (a staple, '110'),\n",
       "  (the coffee, '010'),\n",
       "  (brewed coffee, '110'),\n",
       "  (aroma, '010'),\n",
       "  (The future, '010'),\n",
       "  (coffee beans, '101'),\n",
       "  (the morning, '010'),\n",
       "  (a product, '110'),\n",
       "  (roasted beans, '101'),\n",
       "  (New Orleans, '110'),\n",
       "  (the startup, '011'),\n",
       "  (his first sip, '011'),\n",
       "  (aroma, '010'),\n",
       "  (the product, '010'),\n",
       "  (a market, '110'),\n",
       "  (countless cups, '101'),\n",
       "  (a report, '101'),\n",
       "  (a level, '110'),\n",
       "  (the market, '010'),\n",
       "  (The amount, '001'),\n",
       "  (Washington, '100'),\n",
       "  (A taste test, '111'),\n",
       "  (beekeeper, '110'),\n",
       "  (a product, '110'),\n",
       "  (investors, '010'),\n",
       "  (the compound, '001'),\n",
       "  (Oregon, '101')],\n",
       " 1: [(they, '1'),\n",
       "  (what, '1'),\n",
       "  (we, '1'),\n",
       "  (we, '1'),\n",
       "  (cream, '1'),\n",
       "  (he, '1'),\n",
       "  (foods, '1'),\n",
       "  (who, '1'),\n",
       "  (They, '1'),\n",
       "  (cream, '1'),\n",
       "  (I, '1'),\n",
       "  (We, '1'),\n",
       "  (we, '1'),\n",
       "  (who, '1'),\n",
       "  (Fire, '1'),\n",
       "  (I, '1'),\n",
       "  (We, '1'),\n",
       "  (it, '0'),\n",
       "  (proof, '1'),\n",
       "  (it, '0'),\n",
       "  (it, '0'),\n",
       "  (We, '1'),\n",
       "  (we, '1'),\n",
       "  (it, '0'),\n",
       "  (land, '1'),\n",
       "  (we, '1'),\n",
       "  (we, '1'),\n",
       "  (he, '1'),\n",
       "  (it, '0'),\n",
       "  (we, '1'),\n",
       "  (It, '0'),\n",
       "  (it, '0'),\n",
       "  (we, '1'),\n",
       "  (I, '1'),\n",
       "  (it, '0'),\n",
       "  (we, '1'),\n",
       "  (joe, '1'),\n",
       "  (it, '0'),\n",
       "  (we, '1'),\n",
       "  (she, '1'),\n",
       "  (fact, '1'),\n",
       "  (he, '1'),\n",
       "  (it, '0'),\n",
       "  (we, '1'),\n",
       "  (me, '1'),\n",
       "  (food, '1'),\n",
       "  (We, '1'),\n",
       "  (it, '0'),\n",
       "  (scoop, '1'),\n",
       "  (It, '0'),\n",
       "  (it, '0'),\n",
       "  (who, '1'),\n",
       "  (food, '1'),\n",
       "  (wood, '1'),\n",
       "  (beans, '1'),\n",
       "  (truth, '1'),\n",
       "  (he, '1'),\n",
       "  (grounds, '1')],\n",
       " 9: [(gas and liquid chromatography, '111000100'),\n",
       "  (a North Carolina journalist, '111010100')],\n",
       " 2: [(coffee, '10'),\n",
       "  (coffee, '10'),\n",
       "  (green beans, '11'),\n",
       "  (the team, '01'),\n",
       "  (sugar, '10'),\n",
       "  (farming, '10'),\n",
       "  (that scoop, '01'),\n",
       "  (Hendon, '10'),\n",
       "  (dozens, '10'),\n",
       "  (flavor, '10'),\n",
       "  (demand, '01'),\n",
       "  (coffee, '10'),\n",
       "  (the board, '01'),\n",
       "  (coffee, '10'),\n",
       "  (coffee, '10'),\n",
       "  (coffee, '10'),\n",
       "  (coffee, '10'),\n",
       "  (compounds, '01'),\n",
       "  (something, '10'),\n",
       "  (a way, '11'),\n",
       "  (Kettle, '10'),\n",
       "  (coffee, '10'),\n",
       "  (coffee, '10'),\n",
       "  (caffeine, '01'),\n",
       "  (coffee, '10'),\n",
       "  (four months, '11'),\n",
       "  (coffee, '10'),\n",
       "  (coffee, '10'),\n",
       "  (its blend, '01'),\n",
       "  (the threats, '01'),\n",
       "  (The pair, '01'),\n",
       "  (research, '10'),\n",
       "  (this black, '01'),\n",
       "  (the name, '01'),\n",
       "  (sugar, '10'),\n",
       "  (coffee, '10'),\n",
       "  (history, '10'),\n",
       "  (coffee, '10'),\n",
       "  (coffee, '10'),\n",
       "  (coffee, '10'),\n",
       "  (the bean, '01'),\n",
       "  (labeling, '10'),\n",
       "  (coffee, '10'),\n",
       "  (coffee, '10'),\n",
       "  (a bean, '11'),\n",
       "  (acorns, '10'),\n",
       "  (this cup, '01'),\n",
       "  (coffee, '10'),\n",
       "  (mushrooms, '10'),\n",
       "  (compounds, '01'),\n",
       "  (the fact, '01'),\n",
       "  (people, '10'),\n",
       "  (funding, '10'),\n",
       "  (coffee, '10')],\n",
       " 5: [(the roasted ground root, '01011'),\n",
       "  (the caffeine content, '00101'),\n",
       "  (antioxidants, '10100'),\n",
       "  (the environment, '00100'),\n",
       "  (coffee shortages, '10100'),\n",
       "  (the environment, '00100'),\n",
       "  (its core components, '01010'),\n",
       "  (the difficult space, '01001'),\n",
       "  (a regular cup, '11001'),\n",
       "  (deforestation, '01010'),\n",
       "  (Christopher Hendon, '10010'),\n",
       "  (the food scientist, '01100'),\n",
       "  (an immense power, '00110')],\n",
       " 8: [(Graduate student Taylor Moore, '10110101'),\n",
       "  (the entrepreneurship program, '01001011'),\n",
       "  (the International Center, '01010010'),\n",
       "  (a consistently great coffee, '10100110')],\n",
       " 7: [(an assistant professor, '0010010'),\n",
       "  (the ritual component, '0100010'),\n",
       "  (Tropical Agriculture, '1001010'),\n",
       "  (their science experiment, '1100100'),\n",
       "  (official definition, '0101010')],\n",
       " 6: [(the University, '010100'),\n",
       "  (the popular beverage, '010010'),\n",
       "  (coffee importation, '101010'),\n",
       "  (the University, '010100'),\n",
       "  (a Seattle garage, '101001'),\n",
       "  (the early prototypes, '010101'),\n",
       "  (the total investment, '010010'),\n",
       "  (alternative coffee, '010010')],\n",
       " 13: [(no official regulatory definition, '1010100101010')],\n",
       " 10: [(several inferior concoctions, '1000100010')]}"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_dict['noun_chunks']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now an update to my fill_out_line function into two forms: one to pull from noun-verb-object sections of the dictionary in that order for the long lines and the other to just pull out an adjective-noun or something like that to append to the short lines of the limerick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_word_of_leng_for_line(max_syllables, min_syllables, master_dict, stress_pattern):\n",
    "    word_list = []\n",
    "    for n in master_dict.keys():\n",
    "        if n <= max_syllables and n>min_syllables:\n",
    "            word_list = word_list + master_dict[n]\n",
    "    searching = True\n",
    "    while searching == True:\n",
    "        if len(word_list) == 1:\n",
    "            word = word_list[0]\n",
    "            searching = False\n",
    "        word = word_list[np.random.randint(0,len(word_list))]\n",
    "        stress_test = 0\n",
    "        for i in range(0,len(word[1])):\n",
    "            if stress_pattern[i] == '1':\n",
    "                if word[1][i] != stress_pattern[i]:\n",
    "                    stress_test +=1\n",
    "        if stress_test == 0:\n",
    "            searching = False\n",
    "    return word\n",
    "\n",
    "def fill_out_long_line(syllables_total, master_dict, stress_pattern, line_beginning=[]):\n",
    "    line = [word for word in line_beginning]\n",
    "    syllables_left = syllables_total\n",
    "    stress_pattern = stress_pattern\n",
    "    if len(line) == 0:\n",
    "        word = find_word_of_leng_for_line(syllables_left, 2, master_dict['noun_chunks'], stress_pattern)\n",
    "        line.append(word[0])\n",
    "        syllables_left -= len(word[1])\n",
    "        stress_pattern = stress_pattern[len(word[1])-1:]\n",
    "    if syllables_left >0:\n",
    "        if syllables_left in master_dict['verbs'].keys():\n",
    "            word = find_word_of_leng_for_line(syllables_left,syllables_left-1, master_dict['verbs'], stress_pattern)\n",
    "            line.append(word[0])\n",
    "            syllables_left -= len(word[1])\n",
    "            stress_pattern = stress_pattern[len(word[1])-1:]\n",
    "        else:\n",
    "            word = find_word_of_leng_for_line(syllables_left,syllables_left-2, master_dict['verbs'], stress_pattern)\n",
    "            line.append(word[0])\n",
    "            syllables_left -= len(word[1])\n",
    "            stress_pattern = stress_pattern[len(word[1])-1:]\n",
    "    return line\n",
    "\n",
    "def fill_out_short_line(syllables_total, master_dict, stress_pattern, line_beginning=[]):\n",
    "    line = [word for word in line_beginning]\n",
    "    syllables_left = syllables_total\n",
    "    stress_pattern = stress_pattern\n",
    "    if len(line) == 0:\n",
    "        word = find_word_of_leng_for_line(syllables_left, syllables_left-1, \n",
    "                                          master_dict['noun_chunks'], stress_pattern)\n",
    "        line.append(word[0])\n",
    "        syllables_left -= len(word[1])\n",
    "        stress_pattern = stress_pattern[len(word[1])-1:]\n",
    "    else:\n",
    "        word = find_word_of_leng_for_line(syllables_left, syllables_left-1, \n",
    "                                          master_dict['adjectives'], stress_pattern)\n",
    "        line = [word] + line\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[a report, completed]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_out_long_line(6, master_dict, '10010010', line_beginning=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[coffee beans]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_out_short_line(3, master_dict, '10010010', line_beginning=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_nonsense_limerick(text):\n",
    "    parsed_text = word_parse(text)\n",
    "    a_rhymes, b_rhymes = assign_pos_rhymes(parsed_text)\n",
    "    master_dict = parsed_pronunciation_dict(parsed_text)\n",
    "    a_1 = (a_rhymes[0],robust_pronouncer(a_rhymes[0])['syllables'])\n",
    "    line_1 = fill_out_long_line(8-a_1[1], master_dict, '10010010'[:-a_1[1]])+[a_1[0]]\n",
    "    a_2 = (a_rhymes[1],robust_pronouncer(a_rhymes[1])['syllables'])\n",
    "    line_2 = fill_out_long_line(8-a_2[1], master_dict, '10010010'[:-a_2[1]])+[a_2[0]]\n",
    "    b_1 = (b_rhymes[0],robust_pronouncer(b_rhymes[0])['syllables'])\n",
    "    line_3 = fill_out_short_line(5-b_1[1], master_dict, '10010010'[:-b_1[1]])+[b_1[0]]\n",
    "    b_2 = (b_rhymes[1],robust_pronouncer(b_rhymes[1])['syllables'])\n",
    "    line_4 = fill_out_short_line(5-b_2[1], master_dict, '10010010'[:-b_2[1]])+[b_2[0]]\n",
    "    a_3 = (a_rhymes[2],robust_pronouncer(a_rhymes[2])['syllables'])\n",
    "    line_5 = fill_out_long_line(8-a_3[1], master_dict, '10010010'[:-a_3[1]])+[a_3[0]]\n",
    "    return line_1, line_2,line_3,line_4,line_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([beekeeper, experience, 'round'],\n",
       " [coffee beans, experience, 'ground'],\n",
       " [A taste test, 'waking'],\n",
       " [A concept, 'making'],\n",
       " [a level, expected, 'compound'])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_nonsense_limerick(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is some code to try and plug into various news apis to pull texts in a more automated manner as well as some example limericks. Results are mixed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newsapi import NewsApiClient\n",
    "from newspaper import Article\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nyt_top_api(num_texts):\n",
    "    resp = requests.get('https://api.nytimes.com/svc/mostpopular/v2/viewed/1.json?api-key=xQnzVylvmGHfGWNKzhP6AvSpBTr1fpcK')\n",
    "    data = resp.json()\n",
    "    df = pd.DataFrame(data['results'][0:num_texts])\n",
    "    return df\n",
    "\n",
    "articles = nyt_top_api(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.nytimes.com/interactive/2019/07/06/us/migrants-border-patrol-clint.html'"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles.loc[14]['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(df):\n",
    "    urls = df.url\n",
    "    texts = []\n",
    "    for url in urls:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        t = article.text\n",
    "        texts.append(t)\n",
    "    return pd.DataFrame(texts)\n",
    "\n",
    "texts = get_text(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1130,\n",
       " 985,\n",
       " 1913,\n",
       " 1907,\n",
       " 1196,\n",
       " 2366,\n",
       " 1955,\n",
       " 2286,\n",
       " 1914,\n",
       " 2095,\n",
       " 1425,\n",
       " 2277,\n",
       " 1544,\n",
       " 3691,\n",
       " 24773,\n",
       " 1653,\n",
       " 1051,\n",
       " 1572,\n",
       " 1443,\n",
       " 960]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(text) for text in texts[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([a fixture, equipped, 'director'],\n",
       " [older boys, described, 'inspector'],\n",
       " [a border wall, 'tried'],\n",
       " [instant oatmeal, 'cried'],\n",
       " [their parents, including, 'sector'])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_nonsense_limerick(texts[0][14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([Mexico, equipped, 'inspection'],\n",
       " [nobody, conducted, 'action'],\n",
       " [nobody, 'return'],\n",
       " [her infant son, 'learn'],\n",
       " [Fan Trailer, converted, 'section'])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_nonsense_limerick(texts[0][14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([their parents, expressed, 'inspection'],\n",
       " [Mr. Hull, intended, 'section'],\n",
       " [her infant son, 'tracked'],\n",
       " [A Migrant Jail, 'backed'],\n",
       " [all her years, resemble, 'action'])"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_nonsense_limerick(texts[0][14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['sleep', 'Tents', 'hospital', 'took', 'return'],\n",
       " ['agency', 'Officials', 'held', 'learn'],\n",
       " ['asked', 'floor', 'held', 'table'],\n",
       " ['went', 'said', 'unable'],\n",
       " [('A', 'Forward'), 'said', 'reports', 'concern'])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_nonsense_limerick_simple(texts[0][14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_text = '''\n",
    "Speaker Nancy Pelosi said they have no following in Congress. Representative Alexandria Ocasio-Cortez of New York shot back that she and three of her fellow liberal freshmen, darlings of the left known collectively as “the squad,” are wielding the real power in the party.\n",
    "Six months into the new House Democratic majority, long-simmering tensions between the speaker and the squad — Representatives Ocasio-Cortez, Ilhan Omar of Minnesota, Rashida Tlaib of Michigan and Ayanna Pressley of Massachusetts — have boiled over in the most public of ways, setting off a flurry of criticism of Ms. Pelosi among liberal activists and reinvigorating a debate within the party about how best to stand up to President Trump.\n",
    "The fire was lit by a $4.6 billion border aid package passed by Congress that the quartet argued had empowered Mr. Trump’s immigration crackdown. But the forest already was a tinder box, dried by the monthslong debate over impeachment, earlier dust-ups with Ms. Omar and Ms. Tlaib and over Ms. Ocasio-Cortez’s Green New Deal, and looming debates over a $15-an-hour minimum wage bill and funding for Immigration and Customs Enforcement.\n",
    "The squabble is all the more notable because it pits Ms. Pelosi, the liberal San Francisco congresswoman who is the most powerful elected woman in American history, against a group of progressive Democratic women of color who have broken barriers of their own as part of the most diverse class ever to serve in the House.\n",
    "\"This is an inevitable tension between a few progressives with one priority, which is their ideology, and a speaker with many priorities, including preserving the majority in the House, electing a Democratic president against Trump, and responding to the consensus of her caucus,” said Steve Israel, a Democrat and former representative of New York. “To the extent that it distracts from Donald Trump and becomes a circular firing squad among Democrats, it can be lethal.”\n",
    "Others see an old guard defending itself against powerful young voices demanding change.\n",
    "“Those freshman members are breaking through, and they’re building a movement, and the more power that movement gains, the more persuasive they will be to Pelosi,” said Brian Fallon, a former spokesman for Senator Chuck Schumer and Hillary Clinton.\n",
    "The contretemps began when Maureen Dowd, the New York Times columnist, asked Ms. Pelosi about the squad’s fury over the border aid package. The speaker noted that the group had failed to persuade any other Democrats to join them last month in voting against the House’s version of the bill, which placed restrictions on how the administration could spend the money and demanded standards of care at migrant detention centers.\n",
    "“All these people have their public whatever and their Twitter world,” Ms. Pelosi told Ms. Dowd in an interview published over the weekend by The Times. “But they didn’t have any following. They’re four people, and that’s how many votes they got.”\n",
    "Ms. Ocasio-Cortez, the Queens congresswoman who upset a 20-year Democratic incumbent in a primary and who has carved out a reputation as an outspoken and social-media savvy firebrand in the halls of Congress, responded tartly in a string of Twitter posts — a public show of defiance to the leader of her party 50 years her senior.\n",
    "“That public ‘whatever’ is called public sentiment,” she wrote to her more than 4.7 million followers in a message that was recirculated 10,000 times and “liked” by 65,000 people. “And wielding the power to shift it is how we actually achieve meaningful change in this country.”\n",
    "Ms. Omar chimed in with a tweet of solidarity. “Patetico!” she wrote on her personal Twitter account, with more than 1 million followers. “You know they’re just salty about WHO is wielding the power to shift ‘public sentiment’ these days, sis. Sorry not sorry.”\n",
    "Ms. Ocasio-Cortez’s chief of staff, Saikat Chakrabarti, went much further, arguing in a series of tweets that his boss and her first-term colleagues were better at leading than Ms. Pelosi was, that Democratic leaders were not willing to fight for their principles, and that the speaker had failed to deliver any Democratic victories while shrinking from impeachment proceedings against Mr. Trump.\n",
    "“Pelosi claims we can’t focus on impeachment because it’s a distraction from kitchen table issues,” Mr. Chakrabarti wrote. “But I’d challenge you to find voters that can name a single thing House Democrats have done for their kitchen table this year. What is this legislative mastermind doing?”\n",
    "The back and forth has less to do with ideological differences between Ms. Pelosi and the young crop of progressives than their divergent styles and agendas.\n",
    "Ms. Pelosi, whose legislative triumphs include muscling the Affordable Care Act through the House in 2010, has focused on using the House Democrats’ power to challenge Mr. Trump by advancing legislation that appeals to the broadest possible swath of Democrats, including the more than two dozen moderate lawmakers elected in districts carried by the president in 2016. She has kept the fractious caucus united on measures addressing health care, gun safety, election reforms and immigration, even as divisions persist over whether to impeach Mr. Trump, a step she has so far refused to endorse.\n",
    "The speaker is also giving voice to an undercurrent of resentment among Democratic lawmakers toward Ms. Ocasio-Cortez and her group, whom they see as using their megaphones to sow intraparty divisions and burnish their own brands without achieving any results for Democrats.\n",
    "Ms. Pelosi seemed to allude to that on Monday when she was asked to clarify her remarks to Ms. Dowd.\n",
    "“It wasn’t dismissive; it was a statement of fact,” Ms. Pelosi told a reporter in San Francisco on Monday, saying while most House Democrats had “voted to protect the children” by supporting the House’s humanitarian aid bill, the squad had chosen not to. “They were four who argued against the bill, and they were the only four who voted against the bill. All I said was nobody followed their lead.”\n",
    "“They have a following in the public,” Ms. Pelosi added. “I’m just talking about in the Congress.”\n",
    "The foursome has helped to redefine their party’s message, pushing multi-trillion-dollar ideas like the Green New Deal, Medicare for all, and tuition-free college that have drawn broad rhetorical support, including from Democratic presidential candidates. But they have yet to translate their vision into concrete legislative achievement.\n",
    "The squad and its allies argue that they are tapping into the real energy in the Democratic base with their uncompromising and unapologetic stances.\n",
    "“Representing the movement that actually helped to put everyone in Congress into office and give Pelosi her gavel is a critical role, and they’ve been at the forefront of pushing the boundaries of what is possible in Congress,” said Leah Greenberg, the executive director of Indivisible, a progressive advocacy group.\n",
    "Liberal activists tried to use the speaker’s comments to stoke outrage — and to raise money to mount primaries against incumbent Democrats they deem insufficiently liberal.\n",
    "“AOC and The Squad have changed the entire national debate,” said an email rehashing the spat from the Progressive Change Campaign Committee, which offered a colorful “I STAND WITH AOC” sticker to anyone who donated to their work “electing more AOC’s to Congress.”\n",
    "Mr. Fallon, now the executive director of the grass-roots progressive group Demand Justice, said Ms. Ocasio-Cortez has demonstrated a unique ability to grab the public spotlight for liberal candidates and causes, as she did last week when she visited a migrant detention center in Texas. But Ms. Pelosi has an entirely different mandate, he argued, one that her recent comments may have been designed to subtly convey.\n",
    "“I think more than anything it’s a challenge to this ascending wing of the party, that if they actually want to move beyond being the protest wing and have leadership follow their strategy, that they need to grow their base of support and leave her with no option,” he said.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([their megaphones, simmering, 'years'],\n",
       " [President Trump, focus, 'centers'],\n",
       " [Mr. Trump, 'voted'],\n",
       " [Ms. Omar, 'noted'],\n",
       " [nobody, translate, 'followers'])"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_nonsense_limerick(alt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([a few progressives, argued, 'years'],\n",
       " [a public show, grow, 'followers'],\n",
       " [darlings, 'demanded'],\n",
       " [standards, 'responded'],\n",
       " [meaningful change, follow, 'centers'])"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_nonsense_limerick(alt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([Steve Israel, ascending, 'centers'],\n",
       " [their own brands, appeals, 'followers'],\n",
       " [any results, 'find'],\n",
       " [her gavel, 'designed'],\n",
       " [meaningful change, visited, 'years'])"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_nonsense_limerick(alt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_text2 = '''\n",
    "Ross Perot, Brash Texas Billionaire Who Ran for President, Dies at 89.\n",
    "Ross Perot, the wiry Texas gadfly who made a fortune in computer services, amazed the nation with audacious paramilitary missions to Vietnam and Iran, and ran for president in 1992 and 1996 with populist talk of restoring Norman Rockwell’s America, died on Tuesday at his home in Dallas. He was 89.\n",
    "\n",
    "The cause was leukemia, a family spokesman, James Fuller, said.\n",
    "\n",
    "They called him the man from Texarkana, but he really came out of an era — the Great Depression, World War II and the exuberant postwar years — when boys had paper routes, folks tuned in to the radio and patriots rolled up their sleeves for Uncle Sam and built innovative companies and a powerful nation.\n",
    "\n",
    "“Most people give up just when they’re about to achieve success,” Mr. Perot liked to say. “They quit on the one-yard line. They give up at the last minute of the game one foot from a winning touchdown.”\n",
    "\n",
    "He was no quitter: an Eagle Scout, a Navy officer out of Annapolis, a top I.B.M. salesman, the founder of wildly successful data processing enterprises, a crusader for education and against drugs, a billionaire philanthropist. In 1969, he became a kind of folk hero with a quixotic attempt to fly medicine and food to American prisoners of war in North Vietnam.\n",
    "In 1979 he staged a commando raid that he asserted had freed two of his employees, and thousands of criminals and political prisoners, from captivity in revolutionary Iran.\n",
    "And in 1992 he became one of the most unlikely candidates ever to run for president. He had never held public office, and he seemed all wrong, like a cartoon character sprung to life: an elfin 5 feet 6 inches and 144 pounds, with a 1950s crew cut; a squeaky, nasal country-boy twang; and ears that stuck out like Alfred E. Neuman’s on a Mad magazine cover. Stiff-necked, cantankerous, impetuous, often sentimental, he was given to homespun epigrams: “If you see a snake, just kill it. Don’t appoint a committee on snakes.”\n",
    "\n",
    "Under the banner “United We Stand America,” he spent $65 million of his billions in a campaign that featured innovative half-hour infomercials about himself and his ideas. They were popular, with ratings that sometimes surpassed those of prime-time sitcoms. Ignoring negative newspaper and magazine articles, he laid siege to radio and television talk shows. Switchboards lit up with calls from people wanting to volunteer.\n",
    "\n",
    "Before long, millions were responding to his calls to cut government deficits, red tape and waste, to begin rebuilding the crumbling cities and to restore his vision of America: the small-town life idealized in Rockwell’s homey portraits of ballpark patriotism, barbershop wisdom and flag-draped Main Street, a world away from corrupt Washington.\n",
    "While Mr. Perot had done business with every administration since Lyndon B. Johnson’s, the federal government was one of his favorite targets. Washington, he told its own denizens, “has become a town with sound bites, shell games, handlers, media stuntmen who posture, create images, talk, shoot off Roman candles, but don’t ever accomplish anything. We need deeds, not words, in this city.”\n",
    "\n",
    "Improbably, he surged in the polls while the Republican incumbent, George Bush, and the Democrat, Bill Clinton, trained their fire on each other. Polls showed that Mr. Perot’s support came from across the spectrum, from Democrats and Republicans, conservatives and liberals, mostly from the middle class. Citizen drives got him on the ballot in all 50 states. He was on the cover of Time magazine.\n",
    "\n",
    "But at the peak of his popularity, he unexpectedly dropped out of the race. Months later, he jumped back in, saying his withdrawal had been prompted by Republican “dirty tricks” to sabotage his daughter’s wedding with faked compromising photographs.\n",
    "\n",
    "He did surprisingly well in three presidential debates, often mocking the “gridlock” in Washington. “It’s not the Republicans’ fault, of course, and it’s not the Democrats’ fault,” he said in the second round. “Somewhere out there there’s an extraterrestrial that’s doing this to us, I guess.”\n",
    "On Election Day, Mr. Perot finished with 19 percent of the popular vote — almost 20 million ballots — compared with 38 percent for Mr. Bush and 43 percent for Mr. Clinton. It was the strongest third-party showing since Theodore Roosevelt’s Bull Moose run in 1912.\n",
    "\n",
    "It also led to claims by some Republicans, including the president’s son and future president George W. Bush, that Mr. Perot’s candidacy had cost President Bush a second term — a contention refuted by many political analysts, who pointed to, among other things, exit polls showing that Mr. Perot’s strength had not come disproportionately from defecting Republicans.\n",
    "\n",
    "In 1996, Mr. Perot ran again, this time on the new Reform Party ticket, but he fared poorly. By then the epigrams had paled, and voters suspected that his business strengths, the risk-taking and stubborn autocratic personality, might not serve a president constrained by Congress and public opinion. And by then more was known of Mr. Perot, who could be thin-skinned and meanspirited, who had subjected employees to moral codes and lie detector tests, who was drawn to conspiracy theories and had hired private detectives to chase his suspicions.\n",
    "\n",
    "His candidacy was crippled when a commission refused to let him join debates between President Clinton and the Republican nominee, Senator Bob Dole, on the grounds that he did not have a realistic chance of being elected. He won only 8 percent of the vote. But, as he liked to say, “Failures are like skinned knees: painful but superficial.”\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([populist talk, 'education'],\n",
       " [Uncle Sam, 'administration'],\n",
       " [Mr. Perot, 'rolled'],\n",
       " [North Vietnam, 'told'],\n",
       " [Mr. Perot, winning, 'nation'])"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_nonsense_limerick(alt_text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([images, won, 'education'],\n",
       " [government deficits, 'nation'],\n",
       " [President Bush, 'rolled'],\n",
       " [North Vietnam, 'told'],\n",
       " [a campaign, 'administration'])"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_nonsense_limerick(alt_text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([no quitter, restoring, 'nation'],\n",
       " [Ross Perot, came, 'education'],\n",
       " [Mr. Perot, 'quit'],\n",
       " [President Bush, 'lit'],\n",
       " [a campaign, 'administration'])"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_nonsense_limerick(alt_text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([Senator Bob Dole, cut, 'nation'],\n",
       " [Mr. Perot, 'education'],\n",
       " [Time magazine, 'trained'],\n",
       " [anything, 'constrained'],\n",
       " [other things, 'administration'])"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_nonsense_limerick(alt_text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_articles_text(url):\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    req = session.get(url)\n",
    "    soup = BeautifulSoup(req.text, 'lxml')\n",
    "\n",
    "    paragraph_tags = soup.find_all('p', class_= 'css-exrw3m evys1bk0')\n",
    "    if paragraph_tags == []:\n",
    "        paragraph_tags = soup.find_all('p', itemprop = 'articleBody')\n",
    "\n",
    "    article = ''\n",
    "    for p in paragraph_tags:\n",
    "        article = article + ' ' + p.get_text()\n",
    "\n",
    "    # Clean article replacing unicode characters\n",
    "    article = article.replace(u'\\u2018', u\"'\").replace(u'\\u2019', u\"'\").replace(u'\\u201c', u'\"').replace(u'\\u201d', u'\"')\n",
    "\n",
    "    return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.nytimes.com/2019/09/12/science/solar-energy-power-electricity.html'\n",
    "text = scrape_articles_text(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([a chafing dish, generates, 'day'],\n",
       " [a single light, generate, 'way'],\n",
       " [solar cells, 'tested'],\n",
       " [Raman, 'suggested'],\n",
       " [researchers, developing, 'spray'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_nonsense_limerick(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
