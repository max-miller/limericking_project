# The grand limericking project
This repository contains the workings for every stage of my NLP limericking project. The goal was to create a function that could take in a random news text and produce a limerick about the content of the article.

I was inspired by the, well, inspired twitter account https://twitter.com/Limericking which produces topical news limericks. It also struck me as a sort of a fascinating problem to work on, one that touches on all of the aspects that make working with actual human language so tricky: parsing text, extracting meaning, generating language that both follows rules and appears natural.

I am approaching this project in stages and will continue to update as I work. Roughly, I started out learning by doing, tackling the problems in order of difficulty/complexity. As such, the first part of the project was the simplest. Working with Natural Language Toolkit (NLTK) library, I parsed text into parts of speech, and for pronunciation and generated simple haikus about the topics of the news articles. My final working book and notes can be found in the topical haiku folder. In addition I wrote a blog post with more information on my goals and process over on [Medium](https://medium.com/p/3eb057c8154f).

The next thing I did was expand the haiku concept to the limterick form, taking in text, but this time also finding and assigning rhymes and accounting for strong/weak stresses. First, this was done, like the haiku, semi-randomly, simple pulling random words that had an appropriate number of syllables and didn't break the meter. Then, building on parsing work that my partner for a Flatiron School presentation, Jen McKaig, wrote, I refactored the limericking function to assemble lines in Noun-Verb-Object form. This allowed us to create lines that had a sort of 'grammatical feel', although it's also clear that this sort of approach won't really work in the long run. Grammar is simply too complex to really mimic with hard coded rules like this. The code for this part of the project can be found in the non-sense limerick folder.

With the non-sense/semi-random versions completed, I then started on building up capabilities to parse a new article for meaning, to either extract a topic and key figures or key words or, ideally, produce a one or two sentence summary. The first step I tried was using an unsupervised method like Latent Dirichlet Allocation to group articles together into clusters. This could potentially be useful for a couple of reasons, but my experience applying the LDA-identified topic clusters to my haiku generator yielded mixed results. I wrote about the process in more depth at the [second of my Medium posts](https://towardsdatascience.com/limericking-part-2-topic-modeling-with-lda-45476ab9af15). My notebook can be found in the topic modeling folder.

Topic extraction notebook added, explanation forthcoming.
