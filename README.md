# The grand limericking project
This repository contains the workings for every stage of my NLP limericking project. The goal was to create a function that could take in a random news text and produce a limerick about the content of the article.

I was inspired by the, well, inspired twitter account https://twitter.com/Limericking which produces topical news limericks. It also struck me as a sort of a fascinating problem to work on, one that touches on all of the aspects that make working with actual human language so tricky: parsing text, extracting meaning, generating language that both follows rules and appears natural.

I am approaching this project in stages and will continue to update as I work. Roughly, I started out learning by doing, tackling the problems in order of difficulty/complexity. As such, the first part of the project was the simplest. Working with Natural Language Toolkit (NLTK) library, I parsed text into parts of speech, and for pronunciation and generated simple haikus about the topics of the news articles. My final working book and notes can be found in the topical haiku folder. In addition I wrote a blog post with more information on my goals and process over on [Medium](https://medium.com/p/3eb057c8154f).

The next thing I did was expand the haiku concept to the limterick form, taking in text, but this time also finding and assigning rhymes and accounting for strong/weak stresses. First, this was done, like the haiku, semi-randomly, simple pulling random words that had an appropriate number of syllables and didn't break the meter. Then, building on parsing work that my partner for a Flatiron School presentation, Jen McKaig, wrote, I refactored the limericking function to assemble lines in Noun-Verb-Object form. This allowed us to create lines that had a sort of 'grammatical feel', although it's also clear that this sort of approach won't really work in the long run. Grammar is simply too complex to really mimic with hard coded rules like this. The code for this part of the project can be found in the non-sense limerick folder.

With the non-sense/semi-random versions completed, I then started on building up capabilities to parse a new article for meaning, to either extract a topic and key figures or key words or, ideally, produce a one or two sentence summary. The first step I tried was using an unsupervised method like Latent Dirichlet Allocation to group articles together into clusters. This could potentially be useful for a couple of reasons, but my experience applying the LDA-identified topic clusters to my haiku generator yielded mixed results. I wrote about the process in more depth at the [second of my Medium posts](https://towardsdatascience.com/limericking-part-2-topic-modeling-with-lda-45476ab9af15). My notebook can be found in the topic modeling folder.

A much more successful attempt at text summarization followed. The notion behind the 'topic_extration' notebook (also in the topic modeling folder) was to find sentences within a given text that somehow seemed most 'representative' of the text overall. One way to think about this is to look for sentences that seem most similar to the other sentences in the text - that is, that have many words in common with other sentences in the text. This was done by vectorizing the text sentence by sentence, using cosine-similarity to compare each sentence vector with each other sentence vector, and averaging these values for each sentence to find a single 'mean similarity' value for each sentence. This conceptually simple strategy yields pretty good results (much like a previous strategy of mine to identify the topic of a piece by simply looking at which proper nouns occurred most often worked surprisingly well). [A third Medium post](https://towardsdatascience.com/limericking-part-3-text-summarization-f715841a8765) explains my thinking in more detail.
